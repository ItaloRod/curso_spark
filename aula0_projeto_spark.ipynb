{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KccqfD8ujL3K"
      },
      "source": [
        "# Começando o Trabalho\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyAh7l9hGlHq"
      },
      "source": [
        "## Apache Spark - Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy3ApfOoGlHt"
      },
      "source": [
        "### [Apache Spark](https://spark.apache.org/)\n",
        "\n",
        "Apache Spark é uma plataforma de computação em *cluster* que fornece uma API para programação distribuída para processamento de dados em larga escala, semelhante ao modelo *MapReduce*, mas projetada para ser rápida para consultas interativas e algoritmos iterativos.\n",
        "\n",
        "O Spark permite que você distribua dados e tarefas em clusters com vários nós. Imagine cada nó como um computador separado. A divisão dos dados torna mais fácil o trabalho com conjuntos de dados muito grandes porque cada nó funciona processa apenas uma parte parte do volume total de dados.\n",
        "\n",
        "O Spark é amplamente utilizado em projetos analíticos nas seguintes frentes:\n",
        "\n",
        "- Preparação de dados\n",
        "- Modelos de machine learning\n",
        "- Análise de dados em tempo real"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc3Adto4jWDj"
      },
      "source": [
        "### [PySpark](https://spark.apache.org/docs/3.1.2/api/python/index.html)\n",
        "\n",
        "PySpark é uma interface para Apache Spark em Python. Ele não apenas permite que você escreva aplicativos Spark usando APIs Python, mas também fornece o *shell* PySpark para analisar interativamente seus dados em um ambiente distribuído. O PySpark oferece suporte à maioria dos recursos do Spark, como Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) e Spark Core.\n",
        "\n",
        "<center><img src=\"https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/img-001.png\"/></center>\n",
        "\n",
        "#### Spark SQL e DataFrame\n",
        "\n",
        "Spark SQL é um módulo Spark para processamento de dados estruturados. Ele fornece uma abstração de programação chamada DataFrame e também pode atuar como mecanismo de consulta SQL distribuído.\n",
        "\n",
        "#### Spark Streaming\n",
        "\n",
        "Executando em cima do Spark, o recurso de *streaming* no Apache Spark possibilita o uso de poderosas aplicações interativas e analíticas em *streaming* e dados históricos, enquanto herda a facilidade de uso do Spark e as características de tolerância a falhas.\n",
        "\n",
        "#### Spark MLlib\n",
        "\n",
        "Construído sobre o Spark, MLlib é uma biblioteca de aprendizado de máquina escalonável que fornece um conjunto uniforme de APIs de alto nível que ajudam os usuários a criar e ajustar *pipelines* de aprendizado de máquina práticos.\n",
        "\n",
        "#### Spark Core\n",
        "\n",
        "Spark Core é o mecanismo de execução geral subjacente para a plataforma Spark sobre o qual todas as outras funcionalidades são construídas. Ele fornece um RDD (*Resilient Distributed Dataset*) e recursos de computação na memória."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh-9oWn7GlHw"
      },
      "source": [
        "## Utilizando o Spark no Windows\n",
        "\n",
        "[fonte](https://spark.apache.org/docs/3.1.2/api/python/getting_started/install.html)\n",
        "\n",
        "#### Passo 1 - Instalando o Java\n",
        "\n",
        "O PySpark requer a instalação do Java na versão 7 ou superior. Obtenha a versão mais recente clicando [aqui](https://www.java.com/pt-BR/download/). Para verificar a versão que está instalada em sua máquina execute a seguinte linha de código no seu *prompt*:\n",
        "\n",
        "```\n",
        "java -version\n",
        "```\n",
        "\n",
        "#### Passo 2 - Instalando o Python\n",
        "\n",
        "O Python deve ser instalado em sua versão 2.6 ou superior. Para obter a versão mais recente clique [aqui](https://www.python.org/downloads/windows/). Para verificar a versão do Python que está instalada em sua máquina digite o seguinte comando em seu *prompt*:\n",
        "\n",
        "```\n",
        "python --version\n",
        "```\n",
        "\n",
        "#### Passo 3 - Instalando o Apache Spark \n",
        "\n",
        "Selecione a versão mais estável clicando [aqui](http://spark.apache.org/downloads.html). Na criação deste projeto utilizamos a versão do Spark **3.1.2** e como tipo de pacote selecionamos **Pre-built for Apache Hadoop 2.7**.\n",
        "\n",
        "Para instalar o Apache Spark não é necessário executar um instalador, basta descomprimir os arquivos em uma pasta de sua escolha.\n",
        "\n",
        "<font color=red>Obs.: certifique-se de que o caminho onde os arquivos do Spark foram armazenados não contenham espaços (ex.: **\"C:\\spark\\spark-3.1.2-bin-hadoop2.7\"**).</font>\n",
        "\n",
        "Para testar o funcionamento do Spark execute os comandos abaixo em seu *prompt* de comando. Esses comandos assumem que você extraiu os arquivos do Spark na pasta **\"C:\\spark\\\"**.\n",
        "\n",
        "```\n",
        "cd C:\\spark\\spark-3.1.2-bin-hadoop2.7\n",
        "```\n",
        "\n",
        "```\n",
        "bin\\pyspark\n",
        "```\n",
        "\n",
        "O comando acima inicia o *shell* do PySpark que permite trabalhar interativamente com o Spark.\n",
        "\n",
        "Para sair basta digitar `exit()` e logo depois presionar *Enter*. Para voltar ao *prompt* pressione *Enter* novamente.\n",
        "\n",
        "#### Passo 4 - Instalando o findspark\n",
        "\n",
        "```\n",
        "pip install findspark\n",
        "```\n",
        "\n",
        "#### Passo 5 - Instalando o winutils\n",
        "\n",
        "Os arquivos do Spark não incluem o utilitário **winutils.exe** que é utilizado pelo Spark no Windows. Se não informar onde o Spark deve procurar este utilitário, veremos alguns erros no console e também não conseguiremos executar *scripts* Python utilizando o utilitário `spark-submit`.\n",
        "\n",
        "Faça o [download](https://github.com/steveloughran/winutils) para a versão do Hadoop para a qual sua instalação do Spark foi construída. Em nosso exemplo foi utilizada a [versão 2.7](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin). Faça o *download* apenas do arquivo **winutils.exe**.\n",
        "\n",
        "Crie a pasta **\"hadoop\\bin\"** dentro da pasta que contém os arquivos do Spark (em nosso exemplo **\"C:\\spark\\spark-3.1.2-bin-hadoop2.7\"**) e copie o arquivo **winutils.exe** para dentro desta pasta.\n",
        "\n",
        "Crie duas variáveis de ambiente no seu Windows. A primeira chamada **SPARK_HOME** que aponta para a pasta onde os arquivos Spark foram armazenados (em nosso exemplo **\"C:\\spark\\spark-3.1.2-bin-hadoop2.7\"**). A segunda chamada **HADOOP_HOME** que aponta para **%SPARK_HOME%\\hadoop** (assim podemos modificar **SPARK_HOME** sem precisar alterar **HADOOP_HOME**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_pds0odi1Rj"
      },
      "source": [
        "## Utilizando o Spark no Google Colab\n",
        "\n",
        "Para facilitar o desenvolvimento de nosso projeto neste curso vamos utilizar o Google Colab como ferramenta e para configurar o PySpark basta executar os comandos abaixo na própria célula do seu *notebook*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIZNca7Pjgqf"
      },
      "source": [
        "# Carregamento de Dados\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGNS075GGlH0"
      },
      "source": [
        "## [SparkSession](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.html)\n",
        "\n",
        "O ponto de entrada para programar o Spark com a API Dataset e DataFrame.\n",
        "\n",
        "Uma SparkSession pode ser utilizada para criar DataFrames, registrar DataFrames como tabelas, executar consultas SQL em tabelas, armazenar em cache e ler arquivos parquet. Para criar uma SparkSession, use o seguinte padrão de construtor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yayRGdn_GlH1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/08/13 17:17:56 WARN Utils: Your hostname, MacBook-Pro-de-Paulo.local resolves to a loopback address: 127.0.0.1; using 192.168.68.119 instead (on interface en0)\n",
            "24/08/13 17:17:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/08/13 17:17:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master('local[*]') \\\n",
        "    .appName(\"Iniciando com Spark\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PizPNqbmCuSM"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://192.168.68.119:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Iniciando com Spark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x10d53c2e0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbOcx1oXGlH4"
      },
      "source": [
        "## DataFrames com Spark\n",
        "\n",
        "\n",
        "### Interfaces Spark\n",
        "\n",
        "Existem três interfaces principais do Apache Spark que você deve conhecer: Resilient Distributed Dataset, DataFrame e Dataset.\n",
        "\n",
        "- **Resilient Distributed Dataset**: A primeira abstração do Apache Spark foi o Resilient Distributed Dataset (RDD). É uma interface para uma sequência de objetos de dados que consiste em um ou mais tipos localizados em uma coleção de máquinas (um cluster). Os RDDs podem ser criados de várias maneiras e são a API de “nível mais baixo” disponível. Embora esta seja a estrutura de dados original do Apache Spark, você deve se concentrar na API DataFrame, que é um superconjunto da funcionalidade RDD. A API RDD está disponível nas linguagens Java, Python e Scala.\n",
        "\n",
        "- **DataFrame**: Trata-se de um conceito similar ao DataFrame que você pode estar familiarizado como o pacote pandas do Python e a linguagem R . A API DataFrame está disponível nas linguagens Java, Python, R e Scala.\n",
        "\n",
        "- **Dataset**: uma combinação de DataFrame e RDD. Ele fornece a interface digitada que está disponível em RDDs enquanto fornece a conveniência do DataFrame. A API Dataset está disponível nas linguagens Java e Scala.\n",
        "\n",
        "Em muitos cenários, especialmente com as otimizações de desempenho incorporadas em DataFrames e Datasets, não será necessário trabalhar com RDDs. Mas é importante entender a abstração RDD porque:\n",
        "\n",
        "- O RDD é a infraestrutura subjacente que permite que o Spark seja executado com tanta rapidez e forneça a linhagem de dados.\n",
        "\n",
        "- Se você estiver mergulhando em componentes mais avançados do Spark, pode ser necessário usar RDDs.\n",
        "\n",
        "- As visualizações na Spark UI fazem referência a RDDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZpFfzZN0GlH5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[Nome: string, Idade: string]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = [('Zeca','35'), ('Eva', '29')]\n",
        "colNames = ['Nome', 'Idade']\n",
        "df = spark.createDataFrame(data, colNames)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3APKGkAlGlH5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----+\n",
            "|Nome|Idade|\n",
            "+----+-----+\n",
            "|Zeca|   35|\n",
            "| Eva|   29|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NyBW_ff-JMYs"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Nome</th>\n",
              "      <th>Idade</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Zeca</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Eva</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Nome Idade\n",
              "0  Zeca    35\n",
              "1   Eva    29"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.toPandas()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXVfnTIHGlH3"
      },
      "source": [
        "## Projeto\n",
        "\n",
        "Nosso projeto consiste em ler, manipular, tratar e salvar um conjunto de dados volumosos utilizando como ferramenta o Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCYYVAtjhMa7"
      },
      "source": [
        "## Carregamento de dados\n",
        "\n",
        "### Dados Públicos CNPJ\n",
        "#### Receita Federal\n",
        "\n",
        "> [Empresas](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/empresas.zip)\n",
        "> \n",
        "> [Estabelecimentos](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/estabelecimentos.zip)\n",
        "> \n",
        "> [Sócios](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/socios.zip)\n",
        "\n",
        "[Fonte original dos dados](https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros/consultas/dados-publicos-cnpj)\n",
        "\n",
        "---\n",
        "[property SparkSession.read](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.read.html)\n",
        "\n",
        "[DataFrameReader.csv(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f6bwFADGlH6"
      },
      "source": [
        "### Carregando os dados das empresas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sffAOcMQt_aR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "path = 'data_source/empresas'\n",
        "empresas = spark.read.csv(path, sep=';', inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4RVsaljf8mus"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4585679"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "empresas.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0QOCzKVGlH7"
      },
      "source": [
        "## Faça como eu fiz: Estabelecimentos e Sócios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DXanOaDk1Hc"
      },
      "source": [
        "### Carregando os dados dos estabelecimentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hIwhrJTIGlH7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "estabelecimentos = spark.read.csv('data_source/estabelecimentos', sep=';', inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Hpfe_ApWGlH7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4836219"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estabelecimentos.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdBuLfZkGlH7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOylSh6PGlH8"
      },
      "source": [
        "### Carregando os dados dos sócios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "97PLgACTGlH8"
      },
      "outputs": [],
      "source": [
        "socios = spark.read.csv('data_source/socios', sep=';', inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B5giYMXnGlH8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2046430"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "socios.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94dmXGICGlH8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyeUU1CsGlH9"
      },
      "source": [
        "# Manipulando os Dados\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvN_LyAGlH9"
      },
      "source": [
        "## Operações básicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vc8_B1H8iNkX"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_c0</th>\n",
              "      <th>_c1</th>\n",
              "      <th>_c2</th>\n",
              "      <th>_c3</th>\n",
              "      <th>_c4</th>\n",
              "      <th>_c5</th>\n",
              "      <th>_c6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>306</td>\n",
              "      <td>FRANCAMAR REFRIGERACAO TECNICA S/C LTDA</td>\n",
              "      <td>2240</td>\n",
              "      <td>49</td>\n",
              "      <td>0,00</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1355</td>\n",
              "      <td>BRASILEIRO &amp; OLIVEIRA LTDA</td>\n",
              "      <td>2062</td>\n",
              "      <td>49</td>\n",
              "      <td>0,00</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4820</td>\n",
              "      <td>REGISTRO DE IMOVEIS, TABELIONATO 1 DE NOTAS E ...</td>\n",
              "      <td>3034</td>\n",
              "      <td>32</td>\n",
              "      <td>0,00</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5347</td>\n",
              "      <td>ROSELY APARECIDA MONTEIRO CALTABIANO FREITAS</td>\n",
              "      <td>2135</td>\n",
              "      <td>50</td>\n",
              "      <td>0,00</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6846</td>\n",
              "      <td>BADU E FILHOS TECIDOS LTDA</td>\n",
              "      <td>2062</td>\n",
              "      <td>49</td>\n",
              "      <td>4000,00</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    _c0                                                _c1   _c2  _c3  \\\n",
              "0   306            FRANCAMAR REFRIGERACAO TECNICA S/C LTDA  2240   49   \n",
              "1  1355                         BRASILEIRO & OLIVEIRA LTDA  2062   49   \n",
              "2  4820  REGISTRO DE IMOVEIS, TABELIONATO 1 DE NOTAS E ...  3034   32   \n",
              "3  5347       ROSELY APARECIDA MONTEIRO CALTABIANO FREITAS  2135   50   \n",
              "4  6846                         BADU E FILHOS TECIDOS LTDA  2062   49   \n",
              "\n",
              "       _c4  _c5   _c6  \n",
              "0     0,00    1  None  \n",
              "1     0,00    5  None  \n",
              "2     0,00    5  None  \n",
              "3     0,00    5  None  \n",
              "4  4000,00    1  None  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "empresas.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f7djxh5GlH9"
      },
      "source": [
        "### Renomeando as colunas do DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Hg0w9tDlGlH-"
      },
      "outputs": [],
      "source": [
        "empresasColNames = ['cnpj_basico', 'razao_social_nome_empresarial', 'natureza_juridica', 'qualificacao_do_responsavel', 'capital_social_da_empresa', 'porte_da_empresa', 'ente_federativo_responsavel']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1gYSVAgPk2h8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cnpj_basico',\n",
              " 'razao_social_nome_empresarial',\n",
              " 'natureza_juridica',\n",
              " 'qualificacao_do_responsavel',\n",
              " 'capital_social_da_empresa',\n",
              " 'porte_da_empresa',\n",
              " 'ente_federativo_responsavel']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for index, colName in enumerate(empresasColNames):\n",
        "    empresas = empresas.withColumnRenamed(f\"_c{index}\", colName)\n",
        "empresas.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnpj_basico</th>\n",
              "      <th>razao_social_nome_empresarial</th>\n",
              "      <th>natureza_juridica</th>\n",
              "      <th>qualificacao_do_responsavel</th>\n",
              "      <th>capital_social_da_empresa</th>\n",
              "      <th>porte_da_empresa</th>\n",
              "      <th>ente_federativo_responsavel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>306</td>\n",
              "      <td>FRANCAMAR REFRIGERACAO TECNICA S/C LTDA</td>\n",
              "      <td>2240</td>\n",
              "      <td>49</td>\n",
              "      <td>0,00</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1355</td>\n",
              "      <td>BRASILEIRO &amp; OLIVEIRA LTDA</td>\n",
              "      <td>2062</td>\n",
              "      <td>49</td>\n",
              "      <td>0,00</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4820</td>\n",
              "      <td>REGISTRO DE IMOVEIS, TABELIONATO 1 DE NOTAS E ...</td>\n",
              "      <td>3034</td>\n",
              "      <td>32</td>\n",
              "      <td>0,00</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5347</td>\n",
              "      <td>ROSELY APARECIDA MONTEIRO CALTABIANO FREITAS</td>\n",
              "      <td>2135</td>\n",
              "      <td>50</td>\n",
              "      <td>0,00</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6846</td>\n",
              "      <td>BADU E FILHOS TECIDOS LTDA</td>\n",
              "      <td>2062</td>\n",
              "      <td>49</td>\n",
              "      <td>4000,00</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cnpj_basico                      razao_social_nome_empresarial  \\\n",
              "0          306            FRANCAMAR REFRIGERACAO TECNICA S/C LTDA   \n",
              "1         1355                         BRASILEIRO & OLIVEIRA LTDA   \n",
              "2         4820  REGISTRO DE IMOVEIS, TABELIONATO 1 DE NOTAS E ...   \n",
              "3         5347       ROSELY APARECIDA MONTEIRO CALTABIANO FREITAS   \n",
              "4         6846                         BADU E FILHOS TECIDOS LTDA   \n",
              "\n",
              "   natureza_juridica  qualificacao_do_responsavel capital_social_da_empresa  \\\n",
              "0               2240                           49                      0,00   \n",
              "1               2062                           49                      0,00   \n",
              "2               3034                           32                      0,00   \n",
              "3               2135                           50                      0,00   \n",
              "4               2062                           49                   4000,00   \n",
              "\n",
              "   porte_da_empresa ente_federativo_responsavel  \n",
              "0                 1                        None  \n",
              "1                 5                        None  \n",
              "2                 5                        None  \n",
              "3                 5                        None  \n",
              "4                 1                        None  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "empresas.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QMfPDezDGlH-"
      },
      "outputs": [],
      "source": [
        "estabsColNames = ['cnpj_basico', 'cnpj_ordem', 'cnpj_dv', 'identificador_matriz_filial', 'nome_fantasia', 'situacao_cadastral', 'data_situacao_cadastral', 'motivo_situacao_cadastral', 'nome_da_cidade_no_exterior', 'pais', 'data_de_inicio_atividade', 'cnae_fiscal_principal', 'cnae_fiscal_secundaria', 'tipo_de_logradouro', 'logradouro', 'numero', 'complemento', 'bairro', 'cep', 'uf', 'municipio', 'ddd_1', 'telefone_1', 'ddd_2', 'telefone_2', 'ddd_do_fax', 'fax', 'correio_eletronico', 'situacao_especial', 'data_da_situacao_especial']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "a04WKP_dGlH_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cnpj_basico',\n",
              " 'cnpj_ordem',\n",
              " 'cnpj_dv',\n",
              " 'identificador_matriz_filial',\n",
              " 'nome_fantasia',\n",
              " 'situacao_cadastral',\n",
              " 'data_situacao_cadastral',\n",
              " 'motivo_situacao_cadastral',\n",
              " 'nome_da_cidade_no_exterior',\n",
              " 'pais',\n",
              " 'data_de_inicio_atividade',\n",
              " 'cnae_fiscal_principal',\n",
              " 'cnae_fiscal_secundaria',\n",
              " 'tipo_de_logradouro',\n",
              " 'logradouro',\n",
              " 'numero',\n",
              " 'complemento',\n",
              " 'bairro',\n",
              " 'cep',\n",
              " 'uf',\n",
              " 'municipio',\n",
              " 'ddd_1',\n",
              " 'telefone_1',\n",
              " 'ddd_2',\n",
              " 'telefone_2',\n",
              " 'ddd_do_fax',\n",
              " 'fax',\n",
              " 'correio_eletronico',\n",
              " 'situacao_especial',\n",
              " 'data_da_situacao_especial']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for index, colName in enumerate(estabsColNames):\n",
        "    estabelecimentos = estabelecimentos.withColumnRenamed(f\"_c{index}\", colName)\n",
        "estabelecimentos.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/08/13 17:18:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnpj_basico</th>\n",
              "      <th>cnpj_ordem</th>\n",
              "      <th>cnpj_dv</th>\n",
              "      <th>identificador_matriz_filial</th>\n",
              "      <th>nome_fantasia</th>\n",
              "      <th>situacao_cadastral</th>\n",
              "      <th>data_situacao_cadastral</th>\n",
              "      <th>motivo_situacao_cadastral</th>\n",
              "      <th>nome_da_cidade_no_exterior</th>\n",
              "      <th>pais</th>\n",
              "      <th>...</th>\n",
              "      <th>municipio</th>\n",
              "      <th>ddd_1</th>\n",
              "      <th>telefone_1</th>\n",
              "      <th>ddd_2</th>\n",
              "      <th>telefone_2</th>\n",
              "      <th>ddd_do_fax</th>\n",
              "      <th>fax</th>\n",
              "      <th>correio_eletronico</th>\n",
              "      <th>situacao_especial</th>\n",
              "      <th>data_da_situacao_especial</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1879</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>1</td>\n",
              "      <td>PIRAMIDE M. C.</td>\n",
              "      <td>8</td>\n",
              "      <td>20011029</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7107</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2818</td>\n",
              "      <td>1</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "      <td>20081231</td>\n",
              "      <td>71</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7107</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3110</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "      <td>19971231</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7107</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3733</td>\n",
              "      <td>1</td>\n",
              "      <td>80</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "      <td>20081231</td>\n",
              "      <td>71</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7107</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4628</td>\n",
              "      <td>3</td>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>EMBROIDERY &amp; GIFT</td>\n",
              "      <td>8</td>\n",
              "      <td>19980429</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7075</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   cnpj_basico  cnpj_ordem  cnpj_dv  identificador_matriz_filial  \\\n",
              "0         1879           1       96                            1   \n",
              "1         2818           1       43                            1   \n",
              "2         3110           1        7                            1   \n",
              "3         3733           1       80                            1   \n",
              "4         4628           3       27                            2   \n",
              "\n",
              "       nome_fantasia  situacao_cadastral  data_situacao_cadastral  \\\n",
              "0     PIRAMIDE M. C.                   8                 20011029   \n",
              "1               None                   8                 20081231   \n",
              "2               None                   8                 19971231   \n",
              "3               None                   8                 20081231   \n",
              "4  EMBROIDERY & GIFT                   8                 19980429   \n",
              "\n",
              "   motivo_situacao_cadastral nome_da_cidade_no_exterior  pais  ...  municipio  \\\n",
              "0                          1                       None   NaN  ...       7107   \n",
              "1                         71                       None   NaN  ...       7107   \n",
              "2                          1                       None   NaN  ...       7107   \n",
              "3                         71                       None   NaN  ...       7107   \n",
              "4                          1                       None   NaN  ...       7075   \n",
              "\n",
              "   ddd_1 telefone_1 ddd_2 telefone_2 ddd_do_fax   fax correio_eletronico  \\\n",
              "0   None       None  None       None        NaN  None               None   \n",
              "1   None       None  None       None        NaN  None               None   \n",
              "2   None       None  None       None        NaN  None               None   \n",
              "3   None       None  None       None        NaN  None               None   \n",
              "4   None       None  None       None        NaN  None               None   \n",
              "\n",
              "   situacao_especial data_da_situacao_especial  \n",
              "0               None                       NaN  \n",
              "1               None                       NaN  \n",
              "2               None                       NaN  \n",
              "3               None                       NaN  \n",
              "4               None                       NaN  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estabelecimentos.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-hR0XZghGlH_"
      },
      "outputs": [],
      "source": [
        "sociosColNames = ['cnpj_basico', 'identificador_de_socio', 'nome_do_socio_ou_razao_social', 'cnpj_ou_cpf_do_socio', 'qualificacao_do_socio', 'data_de_entrada_sociedade', 'pais', 'representante_legal', 'nome_do_representante', 'qualificacao_do_representante_legal', 'faixa_etaria']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mjBME0tPGlH_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cnpj_basico',\n",
              " 'identificador_de_socio',\n",
              " 'nome_do_socio_ou_razao_social',\n",
              " 'cnpj_ou_cpf_do_socio',\n",
              " 'qualificacao_do_socio',\n",
              " 'data_de_entrada_sociedade',\n",
              " 'pais',\n",
              " 'representante_legal',\n",
              " 'nome_do_representante',\n",
              " 'qualificacao_do_representante_legal',\n",
              " 'faixa_etaria']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for index, colName in enumerate(sociosColNames):\n",
        "    socios = socios.withColumnRenamed(f\"_c{index}\", colName)\n",
        "socios.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnpj_basico</th>\n",
              "      <th>identificador_de_socio</th>\n",
              "      <th>nome_do_socio_ou_razao_social</th>\n",
              "      <th>cnpj_ou_cpf_do_socio</th>\n",
              "      <th>qualificacao_do_socio</th>\n",
              "      <th>data_de_entrada_sociedade</th>\n",
              "      <th>pais</th>\n",
              "      <th>representante_legal</th>\n",
              "      <th>nome_do_representante</th>\n",
              "      <th>qualificacao_do_representante_legal</th>\n",
              "      <th>faixa_etaria</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>411</td>\n",
              "      <td>2</td>\n",
              "      <td>LILIANA PATRICIA GUASTAVINO</td>\n",
              "      <td>***678188**</td>\n",
              "      <td>22</td>\n",
              "      <td>19940725</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>411</td>\n",
              "      <td>2</td>\n",
              "      <td>CRISTINA HUNDERTMARK</td>\n",
              "      <td>***637848**</td>\n",
              "      <td>28</td>\n",
              "      <td>19940725</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5813</td>\n",
              "      <td>2</td>\n",
              "      <td>CELSO EDUARDO DE CASTRO STEPHAN</td>\n",
              "      <td>***786068**</td>\n",
              "      <td>49</td>\n",
              "      <td>19940516</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5813</td>\n",
              "      <td>2</td>\n",
              "      <td>EDUARDO BERRINGER STEPHAN</td>\n",
              "      <td>***442348**</td>\n",
              "      <td>49</td>\n",
              "      <td>19940516</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14798</td>\n",
              "      <td>2</td>\n",
              "      <td>HANNE MAHFOUD FADEL</td>\n",
              "      <td>***760388**</td>\n",
              "      <td>49</td>\n",
              "      <td>19940609</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cnpj_basico  identificador_de_socio    nome_do_socio_ou_razao_social  \\\n",
              "0          411                       2      LILIANA PATRICIA GUASTAVINO   \n",
              "1          411                       2             CRISTINA HUNDERTMARK   \n",
              "2         5813                       2  CELSO EDUARDO DE CASTRO STEPHAN   \n",
              "3         5813                       2        EDUARDO BERRINGER STEPHAN   \n",
              "4        14798                       2              HANNE MAHFOUD FADEL   \n",
              "\n",
              "  cnpj_ou_cpf_do_socio  qualificacao_do_socio  data_de_entrada_sociedade  \\\n",
              "0          ***678188**                     22                   19940725   \n",
              "1          ***637848**                     28                   19940725   \n",
              "2          ***786068**                     49                   19940516   \n",
              "3          ***442348**                     49                   19940516   \n",
              "4          ***760388**                     49                   19940609   \n",
              "\n",
              "   pais representante_legal nome_do_representante  \\\n",
              "0   NaN         ***000000**                  None   \n",
              "1   NaN         ***000000**                  None   \n",
              "2   NaN         ***000000**                  None   \n",
              "3   NaN         ***000000**                  None   \n",
              "4   NaN         ***000000**                  None   \n",
              "\n",
              "   qualificacao_do_representante_legal  faixa_etaria  \n",
              "0                                    0             7  \n",
              "1                                    0             7  \n",
              "2                                    0             8  \n",
              "3                                    0             5  \n",
              "4                                    0             8  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "socios.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnibIqSUGlH_"
      },
      "source": [
        "## Analisando os dados\n",
        "\n",
        "[Data Types](https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html#data-types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7BCEdVduXqLP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- cnpj_basico: integer (nullable = true)\n",
            " |-- razao_social_nome_empresarial: string (nullable = true)\n",
            " |-- natureza_juridica: integer (nullable = true)\n",
            " |-- qualificacao_do_responsavel: integer (nullable = true)\n",
            " |-- capital_social_da_empresa: string (nullable = true)\n",
            " |-- porte_da_empresa: integer (nullable = true)\n",
            " |-- ente_federativo_responsavel: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empresas.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "aocivOoxGlIA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- cnpj_basico: integer (nullable = true)\n",
            " |-- identificador_de_socio: integer (nullable = true)\n",
            " |-- nome_do_socio_ou_razao_social: string (nullable = true)\n",
            " |-- cnpj_ou_cpf_do_socio: string (nullable = true)\n",
            " |-- qualificacao_do_socio: integer (nullable = true)\n",
            " |-- data_de_entrada_sociedade: integer (nullable = true)\n",
            " |-- pais: integer (nullable = true)\n",
            " |-- representante_legal: string (nullable = true)\n",
            " |-- nome_do_representante: string (nullable = true)\n",
            " |-- qualificacao_do_representante_legal: integer (nullable = true)\n",
            " |-- faixa_etaria: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "socios.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CNQJP7E6UsGC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnpj_basico</th>\n",
              "      <th>identificador_de_socio</th>\n",
              "      <th>nome_do_socio_ou_razao_social</th>\n",
              "      <th>cnpj_ou_cpf_do_socio</th>\n",
              "      <th>qualificacao_do_socio</th>\n",
              "      <th>data_de_entrada_sociedade</th>\n",
              "      <th>pais</th>\n",
              "      <th>representante_legal</th>\n",
              "      <th>nome_do_representante</th>\n",
              "      <th>qualificacao_do_representante_legal</th>\n",
              "      <th>faixa_etaria</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>411</td>\n",
              "      <td>2</td>\n",
              "      <td>LILIANA PATRICIA GUASTAVINO</td>\n",
              "      <td>***678188**</td>\n",
              "      <td>22</td>\n",
              "      <td>19940725</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>411</td>\n",
              "      <td>2</td>\n",
              "      <td>CRISTINA HUNDERTMARK</td>\n",
              "      <td>***637848**</td>\n",
              "      <td>28</td>\n",
              "      <td>19940725</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5813</td>\n",
              "      <td>2</td>\n",
              "      <td>CELSO EDUARDO DE CASTRO STEPHAN</td>\n",
              "      <td>***786068**</td>\n",
              "      <td>49</td>\n",
              "      <td>19940516</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5813</td>\n",
              "      <td>2</td>\n",
              "      <td>EDUARDO BERRINGER STEPHAN</td>\n",
              "      <td>***442348**</td>\n",
              "      <td>49</td>\n",
              "      <td>19940516</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14798</td>\n",
              "      <td>2</td>\n",
              "      <td>HANNE MAHFOUD FADEL</td>\n",
              "      <td>***760388**</td>\n",
              "      <td>49</td>\n",
              "      <td>19940609</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cnpj_basico  identificador_de_socio    nome_do_socio_ou_razao_social  \\\n",
              "0          411                       2      LILIANA PATRICIA GUASTAVINO   \n",
              "1          411                       2             CRISTINA HUNDERTMARK   \n",
              "2         5813                       2  CELSO EDUARDO DE CASTRO STEPHAN   \n",
              "3         5813                       2        EDUARDO BERRINGER STEPHAN   \n",
              "4        14798                       2              HANNE MAHFOUD FADEL   \n",
              "\n",
              "  cnpj_ou_cpf_do_socio  qualificacao_do_socio  data_de_entrada_sociedade  \\\n",
              "0          ***678188**                     22                   19940725   \n",
              "1          ***637848**                     28                   19940725   \n",
              "2          ***786068**                     49                   19940516   \n",
              "3          ***442348**                     49                   19940516   \n",
              "4          ***760388**                     49                   19940609   \n",
              "\n",
              "   pais representante_legal nome_do_representante  \\\n",
              "0   NaN         ***000000**                  None   \n",
              "1   NaN         ***000000**                  None   \n",
              "2   NaN         ***000000**                  None   \n",
              "3   NaN         ***000000**                  None   \n",
              "4   NaN         ***000000**                  None   \n",
              "\n",
              "   qualificacao_do_representante_legal  faixa_etaria  \n",
              "0                                    0             7  \n",
              "1                                    0             7  \n",
              "2                                    0             8  \n",
              "3                                    0             5  \n",
              "4                                    0             8  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "socios.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xUXixnpzZPQe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- cnpj_basico: integer (nullable = true)\n",
            " |-- cnpj_ordem: integer (nullable = true)\n",
            " |-- cnpj_dv: integer (nullable = true)\n",
            " |-- identificador_matriz_filial: integer (nullable = true)\n",
            " |-- nome_fantasia: string (nullable = true)\n",
            " |-- situacao_cadastral: integer (nullable = true)\n",
            " |-- data_situacao_cadastral: integer (nullable = true)\n",
            " |-- motivo_situacao_cadastral: integer (nullable = true)\n",
            " |-- nome_da_cidade_no_exterior: string (nullable = true)\n",
            " |-- pais: integer (nullable = true)\n",
            " |-- data_de_inicio_atividade: integer (nullable = true)\n",
            " |-- cnae_fiscal_principal: integer (nullable = true)\n",
            " |-- cnae_fiscal_secundaria: string (nullable = true)\n",
            " |-- tipo_de_logradouro: string (nullable = true)\n",
            " |-- logradouro: string (nullable = true)\n",
            " |-- numero: string (nullable = true)\n",
            " |-- complemento: string (nullable = true)\n",
            " |-- bairro: string (nullable = true)\n",
            " |-- cep: integer (nullable = true)\n",
            " |-- uf: string (nullable = true)\n",
            " |-- municipio: integer (nullable = true)\n",
            " |-- ddd_1: string (nullable = true)\n",
            " |-- telefone_1: string (nullable = true)\n",
            " |-- ddd_2: string (nullable = true)\n",
            " |-- telefone_2: string (nullable = true)\n",
            " |-- ddd_do_fax: integer (nullable = true)\n",
            " |-- fax: string (nullable = true)\n",
            " |-- correio_eletronico: string (nullable = true)\n",
            " |-- situacao_especial: string (nullable = true)\n",
            " |-- data_da_situacao_especial: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "estabelecimentos.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "43jdqE5gGlIA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnpj_basico</th>\n",
              "      <th>cnpj_ordem</th>\n",
              "      <th>cnpj_dv</th>\n",
              "      <th>identificador_matriz_filial</th>\n",
              "      <th>nome_fantasia</th>\n",
              "      <th>situacao_cadastral</th>\n",
              "      <th>data_situacao_cadastral</th>\n",
              "      <th>motivo_situacao_cadastral</th>\n",
              "      <th>nome_da_cidade_no_exterior</th>\n",
              "      <th>pais</th>\n",
              "      <th>...</th>\n",
              "      <th>municipio</th>\n",
              "      <th>ddd_1</th>\n",
              "      <th>telefone_1</th>\n",
              "      <th>ddd_2</th>\n",
              "      <th>telefone_2</th>\n",
              "      <th>ddd_do_fax</th>\n",
              "      <th>fax</th>\n",
              "      <th>correio_eletronico</th>\n",
              "      <th>situacao_especial</th>\n",
              "      <th>data_da_situacao_especial</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1879</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>1</td>\n",
              "      <td>PIRAMIDE M. C.</td>\n",
              "      <td>8</td>\n",
              "      <td>20011029</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7107</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2818</td>\n",
              "      <td>1</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "      <td>20081231</td>\n",
              "      <td>71</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7107</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3110</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "      <td>19971231</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7107</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3733</td>\n",
              "      <td>1</td>\n",
              "      <td>80</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "      <td>20081231</td>\n",
              "      <td>71</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7107</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4628</td>\n",
              "      <td>3</td>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>EMBROIDERY &amp; GIFT</td>\n",
              "      <td>8</td>\n",
              "      <td>19980429</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7075</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   cnpj_basico  cnpj_ordem  cnpj_dv  identificador_matriz_filial  \\\n",
              "0         1879           1       96                            1   \n",
              "1         2818           1       43                            1   \n",
              "2         3110           1        7                            1   \n",
              "3         3733           1       80                            1   \n",
              "4         4628           3       27                            2   \n",
              "\n",
              "       nome_fantasia  situacao_cadastral  data_situacao_cadastral  \\\n",
              "0     PIRAMIDE M. C.                   8                 20011029   \n",
              "1               None                   8                 20081231   \n",
              "2               None                   8                 19971231   \n",
              "3               None                   8                 20081231   \n",
              "4  EMBROIDERY & GIFT                   8                 19980429   \n",
              "\n",
              "   motivo_situacao_cadastral nome_da_cidade_no_exterior  pais  ...  municipio  \\\n",
              "0                          1                       None   NaN  ...       7107   \n",
              "1                         71                       None   NaN  ...       7107   \n",
              "2                          1                       None   NaN  ...       7107   \n",
              "3                         71                       None   NaN  ...       7107   \n",
              "4                          1                       None   NaN  ...       7075   \n",
              "\n",
              "   ddd_1 telefone_1 ddd_2 telefone_2 ddd_do_fax   fax correio_eletronico  \\\n",
              "0   None       None  None       None        NaN  None               None   \n",
              "1   None       None  None       None        NaN  None               None   \n",
              "2   None       None  None       None        NaN  None               None   \n",
              "3   None       None  None       None        NaN  None               None   \n",
              "4   None       None  None       None        NaN  None               None   \n",
              "\n",
              "   situacao_especial data_da_situacao_especial  \n",
              "0               None                       NaN  \n",
              "1               None                       NaN  \n",
              "2               None                       NaN  \n",
              "3               None                       NaN  \n",
              "4               None                       NaN  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estabelecimentos.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVXz5ZFqY1sW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A75QicR-WYxn"
      },
      "source": [
        "## Modificando os tipos de dados\n",
        "\n",
        "[Functions](https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html#functions)\n",
        "\n",
        "[withColumn](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVElbxNlfFjk"
      },
      "source": [
        "### Convertendo String ➔ Double\n",
        "\n",
        "#### `StringType ➔ DoubleType`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "XSDekv4rbodk"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import DoubleType, StringType\n",
        "from pyspark.sql import functions as f \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tjbORHHw4M5j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- cnpj_basico: integer (nullable = true)\n",
            " |-- razao_social_nome_empresarial: string (nullable = true)\n",
            " |-- natureza_juridica: integer (nullable = true)\n",
            " |-- qualificacao_do_responsavel: integer (nullable = true)\n",
            " |-- capital_social_da_empresa: string (nullable = true)\n",
            " |-- porte_da_empresa: integer (nullable = true)\n",
            " |-- ente_federativo_responsavel: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empresas.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnpj_basico</th>\n",
              "      <th>razao_social_nome_empresarial</th>\n",
              "      <th>natureza_juridica</th>\n",
              "      <th>qualificacao_do_responsavel</th>\n",
              "      <th>capital_social_da_empresa</th>\n",
              "      <th>porte_da_empresa</th>\n",
              "      <th>ente_federativo_responsavel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>306</td>\n",
              "      <td>FRANCAMAR REFRIGERACAO TECNICA S/C LTDA</td>\n",
              "      <td>2240</td>\n",
              "      <td>49</td>\n",
              "      <td>0,00</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1355</td>\n",
              "      <td>BRASILEIRO &amp; OLIVEIRA LTDA</td>\n",
              "      <td>2062</td>\n",
              "      <td>49</td>\n",
              "      <td>0,00</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4820</td>\n",
              "      <td>REGISTRO DE IMOVEIS, TABELIONATO 1 DE NOTAS E ...</td>\n",
              "      <td>3034</td>\n",
              "      <td>32</td>\n",
              "      <td>0,00</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5347</td>\n",
              "      <td>ROSELY APARECIDA MONTEIRO CALTABIANO FREITAS</td>\n",
              "      <td>2135</td>\n",
              "      <td>50</td>\n",
              "      <td>0,00</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6846</td>\n",
              "      <td>BADU E FILHOS TECIDOS LTDA</td>\n",
              "      <td>2062</td>\n",
              "      <td>49</td>\n",
              "      <td>4000,00</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cnpj_basico                      razao_social_nome_empresarial  \\\n",
              "0          306            FRANCAMAR REFRIGERACAO TECNICA S/C LTDA   \n",
              "1         1355                         BRASILEIRO & OLIVEIRA LTDA   \n",
              "2         4820  REGISTRO DE IMOVEIS, TABELIONATO 1 DE NOTAS E ...   \n",
              "3         5347       ROSELY APARECIDA MONTEIRO CALTABIANO FREITAS   \n",
              "4         6846                         BADU E FILHOS TECIDOS LTDA   \n",
              "\n",
              "   natureza_juridica  qualificacao_do_responsavel capital_social_da_empresa  \\\n",
              "0               2240                           49                      0,00   \n",
              "1               2062                           49                      0,00   \n",
              "2               3034                           32                      0,00   \n",
              "3               2135                           50                      0,00   \n",
              "4               2062                           49                   4000,00   \n",
              "\n",
              "   porte_da_empresa ente_federativo_responsavel  \n",
              "0                 1                        None  \n",
              "1                 5                        None  \n",
              "2                 5                        None  \n",
              "3                 5                        None  \n",
              "4                 1                        None  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "empresas.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnpj_basico</th>\n",
              "      <th>razao_social_nome_empresarial</th>\n",
              "      <th>natureza_juridica</th>\n",
              "      <th>qualificacao_do_responsavel</th>\n",
              "      <th>capital_social_da_empresa</th>\n",
              "      <th>porte_da_empresa</th>\n",
              "      <th>ente_federativo_responsavel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>306</td>\n",
              "      <td>FRANCAMAR REFRIGERACAO TECNICA S/C LTDA</td>\n",
              "      <td>2240</td>\n",
              "      <td>49</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1355</td>\n",
              "      <td>BRASILEIRO &amp; OLIVEIRA LTDA</td>\n",
              "      <td>2062</td>\n",
              "      <td>49</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4820</td>\n",
              "      <td>REGISTRO DE IMOVEIS, TABELIONATO 1 DE NOTAS E ...</td>\n",
              "      <td>3034</td>\n",
              "      <td>32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5347</td>\n",
              "      <td>ROSELY APARECIDA MONTEIRO CALTABIANO FREITAS</td>\n",
              "      <td>2135</td>\n",
              "      <td>50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6846</td>\n",
              "      <td>BADU E FILHOS TECIDOS LTDA</td>\n",
              "      <td>2062</td>\n",
              "      <td>49</td>\n",
              "      <td>4000.00</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cnpj_basico                      razao_social_nome_empresarial  \\\n",
              "0          306            FRANCAMAR REFRIGERACAO TECNICA S/C LTDA   \n",
              "1         1355                         BRASILEIRO & OLIVEIRA LTDA   \n",
              "2         4820  REGISTRO DE IMOVEIS, TABELIONATO 1 DE NOTAS E ...   \n",
              "3         5347       ROSELY APARECIDA MONTEIRO CALTABIANO FREITAS   \n",
              "4         6846                         BADU E FILHOS TECIDOS LTDA   \n",
              "\n",
              "   natureza_juridica  qualificacao_do_responsavel capital_social_da_empresa  \\\n",
              "0               2240                           49                      0.00   \n",
              "1               2062                           49                      0.00   \n",
              "2               3034                           32                      0.00   \n",
              "3               2135                           50                      0.00   \n",
              "4               2062                           49                   4000.00   \n",
              "\n",
              "   porte_da_empresa ente_federativo_responsavel  \n",
              "0                 1                        None  \n",
              "1                 5                        None  \n",
              "2                 5                        None  \n",
              "3                 5                        None  \n",
              "4                 1                        None  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "empresas = empresas.withColumn('capital_social_da_empresa', f.regexp_replace('capital_social_da_empresa', ',', '.')) # substitui o , por . em todas a coluna\n",
        "empresas.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EyIP_6ge4ZF4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- cnpj_basico: integer (nullable = true)\n",
            " |-- razao_social_nome_empresarial: string (nullable = true)\n",
            " |-- natureza_juridica: integer (nullable = true)\n",
            " |-- qualificacao_do_responsavel: integer (nullable = true)\n",
            " |-- capital_social_da_empresa: double (nullable = true)\n",
            " |-- porte_da_empresa: integer (nullable = true)\n",
            " |-- ente_federativo_responsavel: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empresas = empresas.withColumn('capital_social_da_empresa', empresas['capital_social_da_empresa'].cast(DoubleType())) # converte a coluna para DoubleType\n",
        "empresas.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUdfkV6EeCJt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsNOpcZoe20V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qUYxtZSGlIE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVxWhP_DZkDC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp_Zv8tAgcbN"
      },
      "source": [
        "### Convertendo String ➔ Date\n",
        "\n",
        "#### `StringType ➔ DateType`\n",
        "\n",
        "[Datetime Patterns](https://spark.apache.org/docs/3.1.2/sql-ref-datetime-pattern.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "iRdCvl26o1eC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20200924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20201022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20210215</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       data\n",
              "0  20200924\n",
              "1  20201022\n",
              "2  20210215"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = spark.createDataFrame([(20200924,), (20201022,), (20210215,)], ['data'])\n",
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_OMaiBT16YAX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- data: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- data: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = df.withColumn('data', f.to_date(df.data.cast(StringType()), 'yyyyMMdd'))\n",
        "df.printSchema()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-09-24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-10-22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-02-15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         data\n",
              "0  2020-09-24\n",
              "1  2020-10-22\n",
              "2  2021-02-15"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- cnpj_basico: integer (nullable = true)\n",
            " |-- cnpj_ordem: integer (nullable = true)\n",
            " |-- cnpj_dv: integer (nullable = true)\n",
            " |-- identificador_matriz_filial: integer (nullable = true)\n",
            " |-- nome_fantasia: string (nullable = true)\n",
            " |-- situacao_cadastral: integer (nullable = true)\n",
            " |-- data_situacao_cadastral: date (nullable = true)\n",
            " |-- motivo_situacao_cadastral: integer (nullable = true)\n",
            " |-- nome_da_cidade_no_exterior: string (nullable = true)\n",
            " |-- pais: integer (nullable = true)\n",
            " |-- data_de_inicio_atividade: date (nullable = true)\n",
            " |-- cnae_fiscal_principal: integer (nullable = true)\n",
            " |-- cnae_fiscal_secundaria: string (nullable = true)\n",
            " |-- tipo_de_logradouro: string (nullable = true)\n",
            " |-- logradouro: string (nullable = true)\n",
            " |-- numero: string (nullable = true)\n",
            " |-- complemento: string (nullable = true)\n",
            " |-- bairro: string (nullable = true)\n",
            " |-- cep: integer (nullable = true)\n",
            " |-- uf: string (nullable = true)\n",
            " |-- municipio: integer (nullable = true)\n",
            " |-- ddd_1: string (nullable = true)\n",
            " |-- telefone_1: string (nullable = true)\n",
            " |-- ddd_2: string (nullable = true)\n",
            " |-- telefone_2: string (nullable = true)\n",
            " |-- ddd_do_fax: integer (nullable = true)\n",
            " |-- fax: string (nullable = true)\n",
            " |-- correio_eletronico: string (nullable = true)\n",
            " |-- situacao_especial: string (nullable = true)\n",
            " |-- data_da_situacao_especial: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "estabelecimentos = estabelecimentos.withColumn( \n",
        "        'data_situacao_cadastral',\n",
        "        f.to_date(estabelecimentos.data_situacao_cadastral.cast(StringType()), 'yyyyMMdd')\n",
        "    ).withColumn(\n",
        "        'data_de_inicio_atividade',\n",
        "        f.to_date(estabelecimentos.data_de_inicio_atividade.cast(StringType()), 'yyyyMMdd')\n",
        "    ).withColumn(\n",
        "        'data_da_situacao_especial',\n",
        "        f.to_date(estabelecimentos.data_da_situacao_especial.cast(StringType()), 'yyyyMMdd')\n",
        "    )\n",
        "\n",
        "estabelecimentos.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnpj_basico</th>\n",
              "      <th>cnpj_ordem</th>\n",
              "      <th>cnpj_dv</th>\n",
              "      <th>identificador_matriz_filial</th>\n",
              "      <th>nome_fantasia</th>\n",
              "      <th>situacao_cadastral</th>\n",
              "      <th>data_situacao_cadastral</th>\n",
              "      <th>motivo_situacao_cadastral</th>\n",
              "      <th>nome_da_cidade_no_exterior</th>\n",
              "      <th>pais</th>\n",
              "      <th>...</th>\n",
              "      <th>municipio</th>\n",
              "      <th>ddd_1</th>\n",
              "      <th>telefone_1</th>\n",
              "      <th>ddd_2</th>\n",
              "      <th>telefone_2</th>\n",
              "      <th>ddd_do_fax</th>\n",
              "      <th>fax</th>\n",
              "      <th>correio_eletronico</th>\n",
              "      <th>situacao_especial</th>\n",
              "      <th>data_da_situacao_especial</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1879</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>1</td>\n",
              "      <td>PIRAMIDE M. C.</td>\n",
              "      <td>8</td>\n",
              "      <td>2001-10-29</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7107</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2818</td>\n",
              "      <td>1</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "      <td>2008-12-31</td>\n",
              "      <td>71</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7107</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3110</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "      <td>1997-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7107</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3733</td>\n",
              "      <td>1</td>\n",
              "      <td>80</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "      <td>2008-12-31</td>\n",
              "      <td>71</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7107</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4628</td>\n",
              "      <td>3</td>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>EMBROIDERY &amp; GIFT</td>\n",
              "      <td>8</td>\n",
              "      <td>1998-04-29</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7075</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   cnpj_basico  cnpj_ordem  cnpj_dv  identificador_matriz_filial  \\\n",
              "0         1879           1       96                            1   \n",
              "1         2818           1       43                            1   \n",
              "2         3110           1        7                            1   \n",
              "3         3733           1       80                            1   \n",
              "4         4628           3       27                            2   \n",
              "\n",
              "       nome_fantasia  situacao_cadastral data_situacao_cadastral  \\\n",
              "0     PIRAMIDE M. C.                   8              2001-10-29   \n",
              "1               None                   8              2008-12-31   \n",
              "2               None                   8              1997-12-31   \n",
              "3               None                   8              2008-12-31   \n",
              "4  EMBROIDERY & GIFT                   8              1998-04-29   \n",
              "\n",
              "   motivo_situacao_cadastral nome_da_cidade_no_exterior  pais  ... municipio  \\\n",
              "0                          1                       None   NaN  ...      7107   \n",
              "1                         71                       None   NaN  ...      7107   \n",
              "2                          1                       None   NaN  ...      7107   \n",
              "3                         71                       None   NaN  ...      7107   \n",
              "4                          1                       None   NaN  ...      7075   \n",
              "\n",
              "   ddd_1 telefone_1 ddd_2 telefone_2 ddd_do_fax   fax correio_eletronico  \\\n",
              "0   None       None  None       None        NaN  None               None   \n",
              "1   None       None  None       None        NaN  None               None   \n",
              "2   None       None  None       None        NaN  None               None   \n",
              "3   None       None  None       None        NaN  None               None   \n",
              "4   None       None  None       None        NaN  None               None   \n",
              "\n",
              "   situacao_especial data_da_situacao_especial  \n",
              "0               None                      None  \n",
              "1               None                      None  \n",
              "2               None                      None  \n",
              "3               None                      None  \n",
              "4               None                      None  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estabelecimentos.limit(5).toPandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- cnpj_basico: integer (nullable = true)\n",
            " |-- identificador_de_socio: integer (nullable = true)\n",
            " |-- nome_do_socio_ou_razao_social: string (nullable = true)\n",
            " |-- cnpj_ou_cpf_do_socio: string (nullable = true)\n",
            " |-- qualificacao_do_socio: integer (nullable = true)\n",
            " |-- data_de_entrada_sociedade: integer (nullable = true)\n",
            " |-- pais: integer (nullable = true)\n",
            " |-- representante_legal: string (nullable = true)\n",
            " |-- nome_do_representante: string (nullable = true)\n",
            " |-- qualificacao_do_representante_legal: integer (nullable = true)\n",
            " |-- faixa_etaria: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "socios.printSchema() # Alterar a data de entrada da sociedade de integer para Date\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- cnpj_basico: integer (nullable = true)\n",
            " |-- identificador_de_socio: integer (nullable = true)\n",
            " |-- nome_do_socio_ou_razao_social: string (nullable = true)\n",
            " |-- cnpj_ou_cpf_do_socio: string (nullable = true)\n",
            " |-- qualificacao_do_socio: integer (nullable = true)\n",
            " |-- data_de_entrada_sociedade: date (nullable = true)\n",
            " |-- pais: integer (nullable = true)\n",
            " |-- representante_legal: string (nullable = true)\n",
            " |-- nome_do_representante: string (nullable = true)\n",
            " |-- qualificacao_do_representante_legal: integer (nullable = true)\n",
            " |-- faixa_etaria: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "socios = socios.withColumn('data_de_entrada_sociedade', f.to_date(socios.data_de_entrada_sociedade.cast(StringType()), 'yyyyMMdd'))\n",
        "socios.printSchema() # Agora a data de entrada na sociedade está como Date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnpj_basico</th>\n",
              "      <th>identificador_de_socio</th>\n",
              "      <th>nome_do_socio_ou_razao_social</th>\n",
              "      <th>cnpj_ou_cpf_do_socio</th>\n",
              "      <th>qualificacao_do_socio</th>\n",
              "      <th>data_de_entrada_sociedade</th>\n",
              "      <th>pais</th>\n",
              "      <th>representante_legal</th>\n",
              "      <th>nome_do_representante</th>\n",
              "      <th>qualificacao_do_representante_legal</th>\n",
              "      <th>faixa_etaria</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>411</td>\n",
              "      <td>2</td>\n",
              "      <td>LILIANA PATRICIA GUASTAVINO</td>\n",
              "      <td>***678188**</td>\n",
              "      <td>22</td>\n",
              "      <td>1994-07-25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>411</td>\n",
              "      <td>2</td>\n",
              "      <td>CRISTINA HUNDERTMARK</td>\n",
              "      <td>***637848**</td>\n",
              "      <td>28</td>\n",
              "      <td>1994-07-25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5813</td>\n",
              "      <td>2</td>\n",
              "      <td>CELSO EDUARDO DE CASTRO STEPHAN</td>\n",
              "      <td>***786068**</td>\n",
              "      <td>49</td>\n",
              "      <td>1994-05-16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5813</td>\n",
              "      <td>2</td>\n",
              "      <td>EDUARDO BERRINGER STEPHAN</td>\n",
              "      <td>***442348**</td>\n",
              "      <td>49</td>\n",
              "      <td>1994-05-16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14798</td>\n",
              "      <td>2</td>\n",
              "      <td>HANNE MAHFOUD FADEL</td>\n",
              "      <td>***760388**</td>\n",
              "      <td>49</td>\n",
              "      <td>1994-06-09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cnpj_basico  identificador_de_socio    nome_do_socio_ou_razao_social  \\\n",
              "0          411                       2      LILIANA PATRICIA GUASTAVINO   \n",
              "1          411                       2             CRISTINA HUNDERTMARK   \n",
              "2         5813                       2  CELSO EDUARDO DE CASTRO STEPHAN   \n",
              "3         5813                       2        EDUARDO BERRINGER STEPHAN   \n",
              "4        14798                       2              HANNE MAHFOUD FADEL   \n",
              "\n",
              "  cnpj_ou_cpf_do_socio  qualificacao_do_socio data_de_entrada_sociedade  pais  \\\n",
              "0          ***678188**                     22                1994-07-25   NaN   \n",
              "1          ***637848**                     28                1994-07-25   NaN   \n",
              "2          ***786068**                     49                1994-05-16   NaN   \n",
              "3          ***442348**                     49                1994-05-16   NaN   \n",
              "4          ***760388**                     49                1994-06-09   NaN   \n",
              "\n",
              "  representante_legal nome_do_representante  \\\n",
              "0         ***000000**                  None   \n",
              "1         ***000000**                  None   \n",
              "2         ***000000**                  None   \n",
              "3         ***000000**                  None   \n",
              "4         ***000000**                  None   \n",
              "\n",
              "   qualificacao_do_representante_legal  faixa_etaria  \n",
              "0                                    0             7  \n",
              "1                                    0             7  \n",
              "2                                    0             8  \n",
              "3                                    0             5  \n",
              "4                                    0             8  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "socios.limit(5).toPandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4alYEILpRZe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxB1k7IGp2vo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yBmkN9H2-QP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgO66CVEUGns"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L2904mRn5dy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39H5_TRgV0j5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE_Hf6G3gak_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTsUeiapO3Pz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rDRJjed8BzS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv529a0SGgMF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg0lldBjGlIB"
      },
      "source": [
        "# Seleções e consultas\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3q8yA8kgH9G"
      },
      "source": [
        "## Selecionando informações\n",
        " \n",
        "[DataFrame.select(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.select.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QdMi5XrjbNxA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------------------------------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
            "|cnpj_basico|razao_social_nome_empresarial                                                               |natureza_juridica|qualificacao_do_responsavel|capital_social_da_empresa|porte_da_empresa|ente_federativo_responsavel|\n",
            "+-----------+--------------------------------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
            "|306        |FRANCAMAR REFRIGERACAO TECNICA S/C LTDA                                                     |2240             |49                         |0.0                      |1               |NULL                       |\n",
            "|1355       |BRASILEIRO & OLIVEIRA LTDA                                                                  |2062             |49                         |0.0                      |5               |NULL                       |\n",
            "|4820       |REGISTRO DE IMOVEIS, TABELIONATO 1 DE NOTAS E TABELIONATO E REGISTRO DE CONSTRATOS MARITIMOS|3034             |32                         |0.0                      |5               |NULL                       |\n",
            "|5347       |ROSELY APARECIDA MONTEIRO CALTABIANO FREITAS                                                |2135             |50                         |0.0                      |5               |NULL                       |\n",
            "|6846       |BADU E FILHOS TECIDOS LTDA                                                                  |2062             |49                         |4000.0                   |1               |NULL                       |\n",
            "+-----------+--------------------------------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empresas.select('*').show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "X9cgyry3GlIB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+----------------+-------------------------+\n",
            "|natureza_juridica|porte_da_empresa|capital_social_da_empresa|\n",
            "+-----------------+----------------+-------------------------+\n",
            "|2240             |1               |0.0                      |\n",
            "|2062             |5               |0.0                      |\n",
            "|3034             |5               |0.0                      |\n",
            "|2135             |5               |0.0                      |\n",
            "|2062             |1               |4000.0                   |\n",
            "+-----------------+----------------+-------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empresas.select('natureza_juridica', 'porte_da_empresa', 'capital_social_da_empresa').show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "7ilkFXsIbw9j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------+------------+--------------+\n",
            "|nome_do_socio_ou_razao_social  |faixa_etaria|ano_de_entrada|\n",
            "+-------------------------------+------------+--------------+\n",
            "|LILIANA PATRICIA GUASTAVINO    |7           |1994          |\n",
            "|CRISTINA HUNDERTMARK           |7           |1994          |\n",
            "|CELSO EDUARDO DE CASTRO STEPHAN|8           |1994          |\n",
            "|EDUARDO BERRINGER STEPHAN      |5           |1994          |\n",
            "|HANNE MAHFOUD FADEL            |8           |1994          |\n",
            "+-------------------------------+------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "socios.select('nome_do_socio_ou_razao_social', 'faixa_etaria', f.year('data_de_entrada_sociedade').alias('ano_de_entrada')).show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j94bdu9r0ykx"
      },
      "source": [
        "## Faça como eu fiz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "smUC0SgN0sGc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+---------+-----------------------+-----------------------+\n",
            "|nome_fantasia    |municipio|ano_de_inicio_atividade|mes_de_inicio_atividade|\n",
            "+-----------------+---------+-----------------------+-----------------------+\n",
            "|PIRAMIDE M. C.   |7107     |1994                   |5                      |\n",
            "|NULL             |7107     |1994                   |5                      |\n",
            "|NULL             |7107     |1994                   |5                      |\n",
            "|NULL             |7107     |1994                   |5                      |\n",
            "|EMBROIDERY & GIFT|7075     |1995                   |5                      |\n",
            "+-----------------+---------+-----------------------+-----------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "estabelecimentos.select(\n",
        "    'nome_fantasia', 'municipio', \n",
        "    f.year('data_de_inicio_atividade').alias('ano_de_inicio_atividade'),  \n",
        "    f.month('data_de_inicio_atividade').alias('mes_de_inicio_atividade')\n",
        "    ).show(5, False)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS3mrMkHZjqX"
      },
      "source": [
        "## Identificando valores nulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "8orupk7E9sfp"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   data\n",
              "0   1.0\n",
              "1   2.0\n",
              "2   3.0\n",
              "3   NaN"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = spark.createDataFrame([(1,), (2,), (3,), (None,)], ['data'])\n",
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4l4asr_S95MN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+\n",
            "|data|\n",
            "+----+\n",
            "|   1|\n",
            "|   2|\n",
            "|   3|\n",
            "|NULL|\n",
            "+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- data: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "UYjUKTUa_KzT"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   data\n",
              "0   1.0\n",
              "1   2.0\n",
              "2   3.0\n",
              "3   NaN"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2 = spark.createDataFrame([(1.,), (2.,), (3.,), (float('nan'),)], ['data'])\n",
        "df2.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- data: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "lqzQebm7_QAO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+\n",
            "|data|\n",
            "+----+\n",
            "|   1|\n",
            "|   2|\n",
            "|   3|\n",
            "|NULL|\n",
            "+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "j8v6WOpb96qU"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   data\n",
              "0     1\n",
              "1     2\n",
              "2     3\n",
              "3  None"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df3 = spark.createDataFrame([('1',), ('2',), ('3',), (None,)], ['data'])\n",
        "df3.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Nokz_PPC994j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+\n",
            "|data|\n",
            "+----+\n",
            "|   1|\n",
            "|   2|\n",
            "|   3|\n",
            "|NULL|\n",
            "+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- data: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df3.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "N1wvIzb8-Rpk"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnpj_basico</th>\n",
              "      <th>identificador_de_socio</th>\n",
              "      <th>nome_do_socio_ou_razao_social</th>\n",
              "      <th>cnpj_ou_cpf_do_socio</th>\n",
              "      <th>qualificacao_do_socio</th>\n",
              "      <th>data_de_entrada_sociedade</th>\n",
              "      <th>pais</th>\n",
              "      <th>representante_legal</th>\n",
              "      <th>nome_do_representante</th>\n",
              "      <th>qualificacao_do_representante_legal</th>\n",
              "      <th>faixa_etaria</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>411</td>\n",
              "      <td>2</td>\n",
              "      <td>LILIANA PATRICIA GUASTAVINO</td>\n",
              "      <td>***678188**</td>\n",
              "      <td>22</td>\n",
              "      <td>1994-07-25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>411</td>\n",
              "      <td>2</td>\n",
              "      <td>CRISTINA HUNDERTMARK</td>\n",
              "      <td>***637848**</td>\n",
              "      <td>28</td>\n",
              "      <td>1994-07-25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5813</td>\n",
              "      <td>2</td>\n",
              "      <td>CELSO EDUARDO DE CASTRO STEPHAN</td>\n",
              "      <td>***786068**</td>\n",
              "      <td>49</td>\n",
              "      <td>1994-05-16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5813</td>\n",
              "      <td>2</td>\n",
              "      <td>EDUARDO BERRINGER STEPHAN</td>\n",
              "      <td>***442348**</td>\n",
              "      <td>49</td>\n",
              "      <td>1994-05-16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14798</td>\n",
              "      <td>2</td>\n",
              "      <td>HANNE MAHFOUD FADEL</td>\n",
              "      <td>***760388**</td>\n",
              "      <td>49</td>\n",
              "      <td>1994-06-09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cnpj_basico  identificador_de_socio    nome_do_socio_ou_razao_social  \\\n",
              "0          411                       2      LILIANA PATRICIA GUASTAVINO   \n",
              "1          411                       2             CRISTINA HUNDERTMARK   \n",
              "2         5813                       2  CELSO EDUARDO DE CASTRO STEPHAN   \n",
              "3         5813                       2        EDUARDO BERRINGER STEPHAN   \n",
              "4        14798                       2              HANNE MAHFOUD FADEL   \n",
              "\n",
              "  cnpj_ou_cpf_do_socio  qualificacao_do_socio data_de_entrada_sociedade  pais  \\\n",
              "0          ***678188**                     22                1994-07-25   NaN   \n",
              "1          ***637848**                     28                1994-07-25   NaN   \n",
              "2          ***786068**                     49                1994-05-16   NaN   \n",
              "3          ***442348**                     49                1994-05-16   NaN   \n",
              "4          ***760388**                     49                1994-06-09   NaN   \n",
              "\n",
              "  representante_legal nome_do_representante  \\\n",
              "0         ***000000**                  None   \n",
              "1         ***000000**                  None   \n",
              "2         ***000000**                  None   \n",
              "3         ***000000**                  None   \n",
              "4         ***000000**                  None   \n",
              "\n",
              "   qualificacao_do_representante_legal  faixa_etaria  \n",
              "0                                    0             7  \n",
              "1                                    0             7  \n",
              "2                                    0             8  \n",
              "3                                    0             5  \n",
              "4                                    0             8  "
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "socios.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "i95DWXnV-Usy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+----+-------------------+---------------------+-----------------------------------+------------+\n",
            "|cnpj_basico|identificador_de_socio|nome_do_socio_ou_razao_social|cnpj_ou_cpf_do_socio|qualificacao_do_socio|data_de_entrada_sociedade|pais|representante_legal|nome_do_representante|qualificacao_do_representante_legal|faixa_etaria|\n",
            "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+----+-------------------+---------------------+-----------------------------------+------------+\n",
            "|        411|                     2|         LILIANA PATRICIA ...|         ***678188**|                   22|               1994-07-25|NULL|        ***000000**|                 NULL|                                  0|           7|\n",
            "|        411|                     2|         CRISTINA HUNDERTMARK|         ***637848**|                   28|               1994-07-25|NULL|        ***000000**|                 NULL|                                  0|           7|\n",
            "|       5813|                     2|         CELSO EDUARDO DE ...|         ***786068**|                   49|               1994-05-16|NULL|        ***000000**|                 NULL|                                  0|           8|\n",
            "|       5813|                     2|         EDUARDO BERRINGER...|         ***442348**|                   49|               1994-05-16|NULL|        ***000000**|                 NULL|                                  0|           5|\n",
            "|      14798|                     2|          HANNE MAHFOUD FADEL|         ***760388**|                   49|               1994-06-09|NULL|        ***000000**|                 NULL|                                  0|           8|\n",
            "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+----+-------------------+---------------------+-----------------------------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "socios.limit(5).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "7f2vmhnQZrod"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 49:>                                                       (0 + 10) / 10]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+-------+-------------------+---------------------+-----------------------------------+------------+\n",
            "|cnpj_basico|identificador_de_socio|nome_do_socio_ou_razao_social|cnpj_ou_cpf_do_socio|qualificacao_do_socio|data_de_entrada_sociedade|   pais|representante_legal|nome_do_representante|qualificacao_do_representante_legal|faixa_etaria|\n",
            "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+-------+-------------------+---------------------+-----------------------------------+------------+\n",
            "|          0|                     0|                          208|                1234|                    0|                        1|2038255|                  0|              1995432|                                  0|           0|\n",
            "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+-------+-------------------+---------------------+-----------------------------------+------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "socios.select([f.count(f.when(f.isnull(c), 1)).alias(c) for c in socios.columns ]).show() # contagem de números nulos por colunas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "nlccfXKMavJv"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnpj_basico</th>\n",
              "      <th>identificador_de_socio</th>\n",
              "      <th>nome_do_socio_ou_razao_social</th>\n",
              "      <th>cnpj_ou_cpf_do_socio</th>\n",
              "      <th>qualificacao_do_socio</th>\n",
              "      <th>data_de_entrada_sociedade</th>\n",
              "      <th>pais</th>\n",
              "      <th>representante_legal</th>\n",
              "      <th>nome_do_representante</th>\n",
              "      <th>qualificacao_do_representante_legal</th>\n",
              "      <th>faixa_etaria</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>411</td>\n",
              "      <td>2</td>\n",
              "      <td>LILIANA PATRICIA GUASTAVINO</td>\n",
              "      <td>***678188**</td>\n",
              "      <td>22</td>\n",
              "      <td>1994-07-25</td>\n",
              "      <td>0</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>411</td>\n",
              "      <td>2</td>\n",
              "      <td>CRISTINA HUNDERTMARK</td>\n",
              "      <td>***637848**</td>\n",
              "      <td>28</td>\n",
              "      <td>1994-07-25</td>\n",
              "      <td>0</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5813</td>\n",
              "      <td>2</td>\n",
              "      <td>CELSO EDUARDO DE CASTRO STEPHAN</td>\n",
              "      <td>***786068**</td>\n",
              "      <td>49</td>\n",
              "      <td>1994-05-16</td>\n",
              "      <td>0</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5813</td>\n",
              "      <td>2</td>\n",
              "      <td>EDUARDO BERRINGER STEPHAN</td>\n",
              "      <td>***442348**</td>\n",
              "      <td>49</td>\n",
              "      <td>1994-05-16</td>\n",
              "      <td>0</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14798</td>\n",
              "      <td>2</td>\n",
              "      <td>HANNE MAHFOUD FADEL</td>\n",
              "      <td>***760388**</td>\n",
              "      <td>49</td>\n",
              "      <td>1994-06-09</td>\n",
              "      <td>0</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cnpj_basico  identificador_de_socio    nome_do_socio_ou_razao_social  \\\n",
              "0          411                       2      LILIANA PATRICIA GUASTAVINO   \n",
              "1          411                       2             CRISTINA HUNDERTMARK   \n",
              "2         5813                       2  CELSO EDUARDO DE CASTRO STEPHAN   \n",
              "3         5813                       2        EDUARDO BERRINGER STEPHAN   \n",
              "4        14798                       2              HANNE MAHFOUD FADEL   \n",
              "\n",
              "  cnpj_ou_cpf_do_socio  qualificacao_do_socio data_de_entrada_sociedade  pais  \\\n",
              "0          ***678188**                     22                1994-07-25     0   \n",
              "1          ***637848**                     28                1994-07-25     0   \n",
              "2          ***786068**                     49                1994-05-16     0   \n",
              "3          ***442348**                     49                1994-05-16     0   \n",
              "4          ***760388**                     49                1994-06-09     0   \n",
              "\n",
              "  representante_legal nome_do_representante  \\\n",
              "0         ***000000**                  None   \n",
              "1         ***000000**                  None   \n",
              "2         ***000000**                  None   \n",
              "3         ***000000**                  None   \n",
              "4         ***000000**                  None   \n",
              "\n",
              "   qualificacao_do_representante_legal  faixa_etaria  \n",
              "0                                    0             7  \n",
              "1                                    0             7  \n",
              "2                                    0             8  \n",
              "3                                    0             5  \n",
              "4                                    0             8  "
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "socios.na.fill(0).limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "p_BjVZKeu_V2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnpj_basico</th>\n",
              "      <th>identificador_de_socio</th>\n",
              "      <th>nome_do_socio_ou_razao_social</th>\n",
              "      <th>cnpj_ou_cpf_do_socio</th>\n",
              "      <th>qualificacao_do_socio</th>\n",
              "      <th>data_de_entrada_sociedade</th>\n",
              "      <th>pais</th>\n",
              "      <th>representante_legal</th>\n",
              "      <th>nome_do_representante</th>\n",
              "      <th>qualificacao_do_representante_legal</th>\n",
              "      <th>faixa_etaria</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>411</td>\n",
              "      <td>2</td>\n",
              "      <td>LILIANA PATRICIA GUASTAVINO</td>\n",
              "      <td>***678188**</td>\n",
              "      <td>22</td>\n",
              "      <td>1994-07-25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>411</td>\n",
              "      <td>2</td>\n",
              "      <td>CRISTINA HUNDERTMARK</td>\n",
              "      <td>***637848**</td>\n",
              "      <td>28</td>\n",
              "      <td>1994-07-25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5813</td>\n",
              "      <td>2</td>\n",
              "      <td>CELSO EDUARDO DE CASTRO STEPHAN</td>\n",
              "      <td>***786068**</td>\n",
              "      <td>49</td>\n",
              "      <td>1994-05-16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5813</td>\n",
              "      <td>2</td>\n",
              "      <td>EDUARDO BERRINGER STEPHAN</td>\n",
              "      <td>***442348**</td>\n",
              "      <td>49</td>\n",
              "      <td>1994-05-16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14798</td>\n",
              "      <td>2</td>\n",
              "      <td>HANNE MAHFOUD FADEL</td>\n",
              "      <td>***760388**</td>\n",
              "      <td>49</td>\n",
              "      <td>1994-06-09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>***000000**</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cnpj_basico  identificador_de_socio    nome_do_socio_ou_razao_social  \\\n",
              "0          411                       2      LILIANA PATRICIA GUASTAVINO   \n",
              "1          411                       2             CRISTINA HUNDERTMARK   \n",
              "2         5813                       2  CELSO EDUARDO DE CASTRO STEPHAN   \n",
              "3         5813                       2        EDUARDO BERRINGER STEPHAN   \n",
              "4        14798                       2              HANNE MAHFOUD FADEL   \n",
              "\n",
              "  cnpj_ou_cpf_do_socio  qualificacao_do_socio data_de_entrada_sociedade  pais  \\\n",
              "0          ***678188**                     22                1994-07-25   NaN   \n",
              "1          ***637848**                     28                1994-07-25   NaN   \n",
              "2          ***786068**                     49                1994-05-16   NaN   \n",
              "3          ***442348**                     49                1994-05-16   NaN   \n",
              "4          ***760388**                     49                1994-06-09   NaN   \n",
              "\n",
              "  representante_legal nome_do_representante  \\\n",
              "0         ***000000**                     -   \n",
              "1         ***000000**                     -   \n",
              "2         ***000000**                     -   \n",
              "3         ***000000**                     -   \n",
              "4         ***000000**                     -   \n",
              "\n",
              "   qualificacao_do_representante_legal  faixa_etaria  \n",
              "0                                    0             7  \n",
              "1                                    0             7  \n",
              "2                                    0             8  \n",
              "3                                    0             5  \n",
              "4                                    0             8  "
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "socios.na.fill('-').limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYp2I6NBaz2C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DrHfT6vbSeU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_NMPijDgp97"
      },
      "source": [
        "## Ordenando os dados\n",
        "\n",
        "[DataFrame.orderBy(*cols, **kwargs)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "RiEEP4eTd9K_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------+------------+--------------+\n",
            "|nome_do_socio_ou_razao_social |faixa_etaria|ano_de_entrada|\n",
            "+------------------------------+------------+--------------+\n",
            "|ANA LUCIA ALVES DE CARVALHO   |5           |2021          |\n",
            "|LUCAS ELIAQUIM CARDOSO SANTANA|3           |2021          |\n",
            "|JOSE HUMBERTO PAIVA           |6           |2021          |\n",
            "|DEUZILENE DE SOUZA SILVA      |5           |2021          |\n",
            "|DOUGLAS CAPPELLETTI           |3           |2021          |\n",
            "+------------------------------+------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "socios.select(\n",
        "    'nome_do_socio_ou_razao_social', \n",
        "    'faixa_etaria', \n",
        "    f.year('data_de_entrada_sociedade').alias('ano_de_entrada')).orderBy(\n",
        "        'ano_de_entrada', ascending=False).show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "PZrd_p7mfFDR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------+------------+--------------+\n",
            "|nome_do_socio_ou_razao_social        |faixa_etaria|ano_de_entrada|\n",
            "+-------------------------------------+------------+--------------+\n",
            "|APPARECIDA ALBANI DE LIMA            |9           |2021          |\n",
            "|MARIA RAIMUNDA DOS SANTOS LANZA      |9           |2021          |\n",
            "|MARIA LUCIA GOMES LOMBA              |9           |2021          |\n",
            "|RENILDE DAS GRACAS MAIA              |9           |2021          |\n",
            "|LUZINETE DANTAS DE OLIVEIRA RODRIGUES|9           |2021          |\n",
            "|DORIS PEREIRA GOMES JAZRA            |9           |2021          |\n",
            "|ANNA BERTAZO PITTON                  |9           |2021          |\n",
            "|MARIA JOSE DOMINGUES BONATO          |9           |2021          |\n",
            "|FRANCISCO ALFREDO LOBO JUNGER        |9           |2021          |\n",
            "|ZELIA MARIA CAMARA RODRIGUES DA SILVA|9           |2021          |\n",
            "+-------------------------------------+------------+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "socios.select(\n",
        "    'nome_do_socio_ou_razao_social', \n",
        "    'faixa_etaria', \n",
        "    f.year('data_de_entrada_sociedade').alias('ano_de_entrada')).orderBy(\n",
        "        'ano_de_entrada', 'faixa_etaria', ascending=[False, False]).show(10, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeMk86UGhZvc"
      },
      "source": [
        "## Filtrando os dados\n",
        "\n",
        "[DataFrame.where(condition)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.where.html) ou [DataFrame.filter(condition)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.filter.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Y6ZHVYBHjPa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
            "|cnpj_basico|razao_social_nome_empresarial       |natureza_juridica|qualificacao_do_responsavel|capital_social_da_empresa|porte_da_empresa|ente_federativo_responsavel|\n",
            "+-----------+------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
            "|17350147   |ERIK MARCELO DOS SANTOS 42107848858 |2135             |50                         |50.0                     |1               |NULL                       |\n",
            "|17833214   |ALEXANDRE MACHADO LIMA 73750123772  |2135             |50                         |50.0                     |1               |NULL                       |\n",
            "|20860830   |YASMIN MOURA DA FONSECA 13457709793 |2135             |50                         |50.0                     |1               |NULL                       |\n",
            "|22242856   |JOAO CESAR MESSIAS 08707149883      |2135             |50                         |50.0                     |1               |NULL                       |\n",
            "|23238540   |EVERTON ROBERTO DA SILVA 42101963809|2135             |50                         |50.0                     |1               |NULL                       |\n",
            "+-----------+------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empresas.where('capital_social_da_empresa==50').show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Z6tE7LiKXT1I"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nome_do_socio_ou_razao_social</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PAULO ANTONIO RODRIGUES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PAULO ROBERTO RODRIGUES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PAULO ROBERTO DE OLIVEIRA RODRIGUES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PAULO ROGERIO RODRIGUES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PAULO JACINTO RODRIGUES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>PAULO SERGIO RODRIGUES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>PAULO ROBERTO RODRIGUES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>PAULO CESAR RODRIGUES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>PAULO DE TARSO RODRIGUES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>PAULO EDUARDO RODRIGUES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         nome_do_socio_ou_razao_social\n",
              "0              PAULO ANTONIO RODRIGUES\n",
              "1              PAULO ROBERTO RODRIGUES\n",
              "2  PAULO ROBERTO DE OLIVEIRA RODRIGUES\n",
              "3              PAULO ROGERIO RODRIGUES\n",
              "4              PAULO JACINTO RODRIGUES\n",
              "5               PAULO SERGIO RODRIGUES\n",
              "6              PAULO ROBERTO RODRIGUES\n",
              "7                PAULO CESAR RODRIGUES\n",
              "8             PAULO DE TARSO RODRIGUES\n",
              "9              PAULO EDUARDO RODRIGUES"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "socios.select('nome_do_socio_ou_razao_social').filter(\n",
        "    socios.nome_do_socio_ou_razao_social.startswith(\"PAULO\")\n",
        "    ).filter(\n",
        "        socios.nome_do_socio_ou_razao_social.endswith(\"RODRIGUES\")\n",
        "    ).limit(10).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ_FrfbOK-1a"
      },
      "source": [
        "## O comando LIKE\n",
        "\n",
        "[Column.like(other)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.Column.like.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "R3sMWIEIQLY7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RESTAURANTE DO RUI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Juca restaurantes ltda</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Joca Restaurante</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     data\n",
              "0      RESTAURANTE DO RUI\n",
              "1  Juca restaurantes ltda\n",
              "2        Joca Restaurante"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = spark.createDataFrame([('RESTAURANTE DO RUI',), ('Juca restaurantes ltda',), ('Joca Restaurante',)], ['data'])\n",
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Zfc1p_pEQ1UJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------+\n",
            "|data                  |\n",
            "+----------------------+\n",
            "|RESTAURANTE DO RUI    |\n",
            "|Juca restaurantes ltda|\n",
            "|Joca Restaurante      |\n",
            "+----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.where(f.upper(df.data).like('%RESTAURANTE%')).show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------+-----------------+----------------+-------------------------+\n",
            "|razao_social_nome_empresarial                        |natureza_juridica|porte_da_empresa|capital_social_da_empresa|\n",
            "+-----------------------------------------------------+-----------------+----------------+-------------------------+\n",
            "|CONTATOS BAR E LANCHONETE LTDA                       |2062             |5               |0.0                      |\n",
            "|BAR E MERCEARIA KIT LTDA                             |2062             |5               |0.0                      |\n",
            "|TRANSPORTADORA BARAO LTDA                            |2062             |5               |0.0                      |\n",
            "|ANTONIO DA FONSECA GONCALVES BAR                     |2135             |5               |0.0                      |\n",
            "|BARCLAYS BRASIL LTDA                                 |2062             |5               |0.0                      |\n",
            "|CAMBARA INFORMATICA SOCIEDADE EMPRESARIA LTDA        |2062             |1               |0.0                      |\n",
            "|SOCIEDADE BENEFICENTE DA BARRA DA TIJUCA             |3999             |5               |0.0                      |\n",
            "|BARROS & MIRANDA LTDA                                |2062             |3               |30000.0                  |\n",
            "|BAR E SNOOKER BALALAIKA LTDA                         |2062             |5               |0.0                      |\n",
            "|EULALIO CUBAR                                        |2135             |5               |0.0                      |\n",
            "|BAR E LANCHONETE TO QUE TO LTDA                      |2062             |5               |0.0                      |\n",
            "|BAR E MERCEARIA PARATI DE TUPA LTDA                  |2062             |1               |0.0                      |\n",
            "|Z.MONTEIRO BAR E MERCEARIA                           |2135             |5               |0.0                      |\n",
            "|MARIA DE LOURDES DE SOUZA BARBOSA                    |2135             |1               |0.0                      |\n",
            "|SOCIEDADE CORPO DE BOMBEIROS VOLUNTARIOS DE BARREIROS|3999             |5               |0.0                      |\n",
            "+-----------------------------------------------------+-----------------+----------------+-------------------------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/08/13 17:18:09 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
          ]
        }
      ],
      "source": [
        "empresas.select(\n",
        "    'razao_social_nome_empresarial', \n",
        "    'natureza_juridica', \n",
        "    'porte_da_empresa', \n",
        "    'capital_social_da_empresa'\n",
        "    ).filter(f.upper(empresas['razao_social_nome_empresarial']).like('%BAR%')).show(15, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHqcmjIxGlIF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiyZCFaEQPtQ"
      },
      "source": [
        "# Agregações e Junções\n",
        "---\n",
        "\n",
        "[DataFrame.groupBy(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html)\n",
        "\n",
        "[DataFrame.agg(*exprs)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.agg.html)\n",
        "\n",
        "[DataFrame.summary(*statistics)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.summary.html)\n",
        "\n",
        "> Funções:\n",
        "[approx_count_distinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.approx_count_distinct.html) | \n",
        "[avg](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.avg.html) | \n",
        "[collect_list](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.collect_list.html) | \n",
        "[collect_set](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.collect_set.html) | \n",
        "[countDistinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.countDistinct.html) | \n",
        "[count](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.count.html) | \n",
        "[grouping](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.grouping.html) | \n",
        "[first](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.first.html) | \n",
        "[last](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.last.html) | \n",
        "[kurtosis](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.kurtosis.html) | \n",
        "[max](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.max.html) | \n",
        "[min](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.min.html) | \n",
        "[mean](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.mean.html) | \n",
        "[skewness](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.skewness.html) | \n",
        "[stddev ou stddev_samp](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.stddev.html) | \n",
        "[stddev_pop](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.stddev_pop.html) | \n",
        "[sum](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.sum.html) | \n",
        "[sumDistinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.sumDistinct.html) | \n",
        "[variance ou var_samp](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.variance.html) | \n",
        "[var_pop](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.var_pop.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc6YDFOkywQc"
      },
      "source": [
        "## Sumarizando os dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "o8p8BS7QQKfS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+------+\n",
            "|ano_de_entrada| count|\n",
            "+--------------+------+\n",
            "|          2010| 79337|\n",
            "|          2011| 83906|\n",
            "|          2012| 80101|\n",
            "|          2013| 83919|\n",
            "|          2014| 80590|\n",
            "|          2015| 80906|\n",
            "|          2016| 81587|\n",
            "|          2017| 90221|\n",
            "|          2018| 99935|\n",
            "|          2019|118248|\n",
            "|          2020|125927|\n",
            "|          2021| 56316|\n",
            "+--------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "socios.select(\n",
        "    f.year('data_de_entrada_sociedade'\n",
        "           ).alias(\n",
        "               'ano_de_entrada'\n",
        "               )).where(\n",
        "                   'ano_de_entrada >= 2010'\n",
        "                   ).groupBy(\n",
        "                       'ano_de_entrada'\n",
        "                       ).count().orderBy(\n",
        "                           'ano_de_entrada', ascending=True).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "xlAbbR4SdZHw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 69:>                                                       (0 + 10) / 10]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+--------------------+----------+\n",
            "|porte_da_empresa|media_capital_social|frequencia|\n",
            "+----------------+--------------------+----------+\n",
            "|            NULL|    8.35421888053467|      5985|\n",
            "|               1|  339994.53313507047|   3129043|\n",
            "|               3|  2601001.7677092687|    115151|\n",
            "|               5|   708660.4208249793|   1335500|\n",
            "+----------------+--------------------+----------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "empresas.select(\n",
        "    'cnpj_basico', \n",
        "    'porte_da_empresa', \n",
        "    'capital_social_da_empresa'\n",
        "    ).groupBy('porte_da_empresa').agg(\n",
        "        f.avg('capital_social_da_empresa').alias('media_capital_social'),\n",
        "        f.count('cnpj_basico').alias('frequencia')\n",
        "    ).orderBy('porte_da_empresa', ascending=True).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "_oOOQPUqNiRZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 72:=====>                                                   (1 + 9) / 10]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------------------------+\n",
            "|summary|capital_social_da_empresa|\n",
            "+-------+-------------------------+\n",
            "|  count|                  4585679|\n",
            "|   mean|        503694.5478542674|\n",
            "| stddev|     2.1118691490537727E8|\n",
            "|    min|                      0.0|\n",
            "|    25%|                      0.0|\n",
            "|    50%|                   1000.0|\n",
            "|    75%|                   7000.0|\n",
            "|    max|         3.22014670262E11|\n",
            "+-------+-------------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "empresas.select('capital_social_da_empresa').summary().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+----+\n",
            "|   nome|   materia|nota|\n",
            "+-------+----------+----+\n",
            "| CARLOS|MATEMÁTICA|   7|\n",
            "|    IVO|MATEMÁTICA|   9|\n",
            "| MÁRCIA|MATEMÁTICA|   8|\n",
            "|  LEILA|MATEMÁTICA|   9|\n",
            "|  BRENO|MATEMÁTICA|   7|\n",
            "|LETÍCIA|MATEMÁTICA|   8|\n",
            "| CARLOS|    FÍSICA|   2|\n",
            "|    IVO|    FÍSICA|   8|\n",
            "| MÁRCIA|    FÍSICA|  10|\n",
            "|  LEILA|    FÍSICA|   9|\n",
            "|  BRENO|    FÍSICA|   1|\n",
            "|LETÍCIA|    FÍSICA|   6|\n",
            "| CARLOS|   QUÍMICA|  10|\n",
            "|    IVO|   QUÍMICA|   8|\n",
            "| MÁRCIA|   QUÍMICA|   1|\n",
            "|  LEILA|   QUÍMICA|  10|\n",
            "|  BRENO|   QUÍMICA|   7|\n",
            "|LETÍCIA|   QUÍMICA|   9|\n",
            "+-------+----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# When / Otherwise\n",
        "\n",
        "data = [\n",
        "    ('CARLOS', 'MATEMÁTICA', 7), \n",
        "    ('IVO', 'MATEMÁTICA', 9), \n",
        "    ('MÁRCIA', 'MATEMÁTICA', 8), \n",
        "    ('LEILA', 'MATEMÁTICA', 9), \n",
        "    ('BRENO', 'MATEMÁTICA', 7), \n",
        "    ('LETÍCIA', 'MATEMÁTICA', 8), \n",
        "    ('CARLOS', 'FÍSICA', 2), \n",
        "    ('IVO', 'FÍSICA', 8), \n",
        "    ('MÁRCIA', 'FÍSICA', 10), \n",
        "    ('LEILA', 'FÍSICA', 9), \n",
        "    ('BRENO', 'FÍSICA', 1), \n",
        "    ('LETÍCIA', 'FÍSICA', 6), \n",
        "    ('CARLOS', 'QUÍMICA', 10), \n",
        "    ('IVO', 'QUÍMICA', 8), \n",
        "    ('MÁRCIA', 'QUÍMICA', 1), \n",
        "    ('LEILA', 'QUÍMICA', 10), \n",
        "    ('BRENO', 'QUÍMICA', 7), \n",
        "    ('LETÍCIA', 'QUÍMICA', 9)\n",
        "]\n",
        "colNames = ['nome', 'materia', 'nota']\n",
        "df = spark.createDataFrame(data, colNames)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+----+---------+\n",
            "|   nome|   materia|nota|   status|\n",
            "+-------+----------+----+---------+\n",
            "| CARLOS|MATEMÁTICA|   7| APROVADO|\n",
            "|    IVO|MATEMÁTICA|   9| APROVADO|\n",
            "| MÁRCIA|MATEMÁTICA|   8| APROVADO|\n",
            "|  LEILA|MATEMÁTICA|   9| APROVADO|\n",
            "|  BRENO|MATEMÁTICA|   7| APROVADO|\n",
            "|LETÍCIA|MATEMÁTICA|   8| APROVADO|\n",
            "| CARLOS|    FÍSICA|   2|REPROVADO|\n",
            "|    IVO|    FÍSICA|   8| APROVADO|\n",
            "| MÁRCIA|    FÍSICA|  10| APROVADO|\n",
            "|  LEILA|    FÍSICA|   9| APROVADO|\n",
            "|  BRENO|    FÍSICA|   1|REPROVADO|\n",
            "|LETÍCIA|    FÍSICA|   6|REPROVADO|\n",
            "| CARLOS|   QUÍMICA|  10| APROVADO|\n",
            "|    IVO|   QUÍMICA|   8| APROVADO|\n",
            "| MÁRCIA|   QUÍMICA|   1|REPROVADO|\n",
            "|  LEILA|   QUÍMICA|  10| APROVADO|\n",
            "|  BRENO|   QUÍMICA|   7| APROVADO|\n",
            "|LETÍCIA|   QUÍMICA|   9| APROVADO|\n",
            "+-------+----------+----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = df.withColumn('status', f.when(df.nota >= 7, \"APROVADO\").otherwise(\"REPROVADO\"))\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----+\n",
            "|summary|nota|\n",
            "+-------+----+\n",
            "|    min|   1|\n",
            "|    25%|   7|\n",
            "|    50%|   8|\n",
            "|    75%|   9|\n",
            "|    max|  10|\n",
            "+-------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select('nota').summary('min', '25%', '50%', '75%', 'max').show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|   status|count|\n",
            "+---------+-----+\n",
            "| APROVADO|   14|\n",
            "|REPROVADO|    4|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy('status').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOCWSMepX5jN"
      },
      "source": [
        "## Juntando DataFrames - Joins\n",
        "\n",
        "[DataFrame.join(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "6uklRCkDX4Tf"
      },
      "outputs": [],
      "source": [
        "produtos = spark.createDataFrame(\n",
        "    [ \n",
        "        ('1', 'Bebidas', 'Água Mineral'),\n",
        "        ('2', 'Limpeza', 'Sabão em Pó'),\n",
        "        ('3', 'Frios', 'Queijo'),\n",
        "        ('4', 'Bebidas', 'Refrigerante'),\n",
        "        ('5', 'Pet', 'Ração para Cães'),\n",
        "        ('6', 'Bebidas', 'Cerveja'),\n",
        "        ('7', 'Bebidas', 'Vinho'),\n",
        "        ('8', 'Frios', 'Presunto'),\n",
        "        ('9', 'Pet', 'Ração para Gatos'),\n",
        "        ('10', 'Frios', 'Mortadela')\n",
        "    ], ['id', 'categoria', 'produto']\n",
        "    )\n",
        "\n",
        "impostos = spark.createDataFrame( \n",
        "    [\n",
        "        ('Bebidas', 0.15),\n",
        "        ('Limpeza', 0.05),\n",
        "        ('Frios', 0.065),\n",
        "        ('Carnes', 0.08)\n",
        "    ], ['categoria', 'taxa']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "eCMHBCrokoc3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>categoria</th>\n",
              "      <th>produto</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Bebidas</td>\n",
              "      <td>Água Mineral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Limpeza</td>\n",
              "      <td>Sabão em Pó</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Frios</td>\n",
              "      <td>Queijo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Bebidas</td>\n",
              "      <td>Refrigerante</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Pet</td>\n",
              "      <td>Ração para Cães</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>Bebidas</td>\n",
              "      <td>Cerveja</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>Bebidas</td>\n",
              "      <td>Vinho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>Frios</td>\n",
              "      <td>Presunto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>Pet</td>\n",
              "      <td>Ração para Gatos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>Frios</td>\n",
              "      <td>Mortadela</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id categoria           produto\n",
              "0   1   Bebidas      Água Mineral\n",
              "1   2   Limpeza       Sabão em Pó\n",
              "2   3     Frios            Queijo\n",
              "3   4   Bebidas      Refrigerante\n",
              "4   5       Pet   Ração para Cães\n",
              "5   6   Bebidas           Cerveja\n",
              "6   7   Bebidas             Vinho\n",
              "7   8     Frios          Presunto\n",
              "8   9       Pet  Ração para Gatos\n",
              "9  10     Frios         Mortadela"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "produtos.toPandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categoria</th>\n",
              "      <th>taxa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bebidas</td>\n",
              "      <td>0.150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Limpeza</td>\n",
              "      <td>0.050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Frios</td>\n",
              "      <td>0.065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Carnes</td>\n",
              "      <td>0.080</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  categoria   taxa\n",
              "0   Bebidas  0.150\n",
              "1   Limpeza  0.050\n",
              "2     Frios  0.065\n",
              "3    Carnes  0.080"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "impostos.toPandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---+------------+-----+\n",
            "|categoria| id|     produto| taxa|\n",
            "+---------+---+------------+-----+\n",
            "|  Bebidas|  1|Água Mineral| 0.15|\n",
            "|    Frios| 10|   Mortadela|0.065|\n",
            "|  Limpeza|  2| Sabão em Pó| 0.05|\n",
            "|    Frios|  3|      Queijo|0.065|\n",
            "|  Bebidas|  4|Refrigerante| 0.15|\n",
            "|  Bebidas|  6|     Cerveja| 0.15|\n",
            "|  Bebidas|  7|       Vinho| 0.15|\n",
            "|    Frios|  8|    Presunto|0.065|\n",
            "+---------+---+------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "produtos.join(impostos, 'categoria', how='inner').sort('id').show() # Interseção entre os dois dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "oDaN4XFQkdsE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---+----------------+-----+\n",
            "|categoria| id|         produto| taxa|\n",
            "+---------+---+----------------+-----+\n",
            "|  Bebidas|  1|    Água Mineral| 0.15|\n",
            "|    Frios| 10|       Mortadela|0.065|\n",
            "|  Limpeza|  2|     Sabão em Pó| 0.05|\n",
            "|    Frios|  3|          Queijo|0.065|\n",
            "|  Bebidas|  4|    Refrigerante| 0.15|\n",
            "|      Pet|  5| Ração para Cães| NULL|\n",
            "|  Bebidas|  6|         Cerveja| 0.15|\n",
            "|  Bebidas|  7|           Vinho| 0.15|\n",
            "|    Frios|  8|        Presunto|0.065|\n",
            "|      Pet|  9|Ração para Gatos| NULL|\n",
            "+---------+---+----------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "produtos.join(impostos, 'categoria', how='left').sort('id').show() # Todos os registros do dataframe da esquerda e os registros que tem correspondência no dataframe da direita"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "VyvClbqRHviC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----+----------------+-----+\n",
            "|categoria|  id|         produto| taxa|\n",
            "+---------+----+----------------+-----+\n",
            "|   Carnes|NULL|            NULL| 0.08|\n",
            "|  Bebidas|   1|    Água Mineral| 0.15|\n",
            "|    Frios|  10|       Mortadela|0.065|\n",
            "|  Limpeza|   2|     Sabão em Pó| 0.05|\n",
            "|    Frios|   3|          Queijo|0.065|\n",
            "|  Bebidas|   4|    Refrigerante| 0.15|\n",
            "|      Pet|   5| Ração para Cães| NULL|\n",
            "|  Bebidas|   6|         Cerveja| 0.15|\n",
            "|  Bebidas|   7|           Vinho| 0.15|\n",
            "|    Frios|   8|        Presunto|0.065|\n",
            "|      Pet|   9|Ração para Gatos| NULL|\n",
            "+---------+----+----------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "produtos.join(impostos, 'categoria', how='outer').sort('id').show() # Todos os registros dos dois dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "nq_pGKucIjri"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- cnpj_basico: integer (nullable = true)\n",
            " |-- cnpj_ordem: integer (nullable = true)\n",
            " |-- cnpj_dv: integer (nullable = true)\n",
            " |-- identificador_matriz_filial: integer (nullable = true)\n",
            " |-- nome_fantasia: string (nullable = true)\n",
            " |-- situacao_cadastral: integer (nullable = true)\n",
            " |-- data_situacao_cadastral: date (nullable = true)\n",
            " |-- motivo_situacao_cadastral: integer (nullable = true)\n",
            " |-- nome_da_cidade_no_exterior: string (nullable = true)\n",
            " |-- pais: integer (nullable = true)\n",
            " |-- data_de_inicio_atividade: date (nullable = true)\n",
            " |-- cnae_fiscal_principal: integer (nullable = true)\n",
            " |-- cnae_fiscal_secundaria: string (nullable = true)\n",
            " |-- tipo_de_logradouro: string (nullable = true)\n",
            " |-- logradouro: string (nullable = true)\n",
            " |-- numero: string (nullable = true)\n",
            " |-- complemento: string (nullable = true)\n",
            " |-- bairro: string (nullable = true)\n",
            " |-- cep: integer (nullable = true)\n",
            " |-- uf: string (nullable = true)\n",
            " |-- municipio: integer (nullable = true)\n",
            " |-- ddd_1: string (nullable = true)\n",
            " |-- telefone_1: string (nullable = true)\n",
            " |-- ddd_2: string (nullable = true)\n",
            " |-- telefone_2: string (nullable = true)\n",
            " |-- ddd_do_fax: integer (nullable = true)\n",
            " |-- fax: string (nullable = true)\n",
            " |-- correio_eletronico: string (nullable = true)\n",
            " |-- situacao_especial: string (nullable = true)\n",
            " |-- data_da_situacao_especial: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "estabelecimentos.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "wWoVxvYKJkC-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- cnpj_basico: integer (nullable = true)\n",
            " |-- identificador_de_socio: integer (nullable = true)\n",
            " |-- nome_do_socio_ou_razao_social: string (nullable = true)\n",
            " |-- cnpj_ou_cpf_do_socio: string (nullable = true)\n",
            " |-- qualificacao_do_socio: integer (nullable = true)\n",
            " |-- data_de_entrada_sociedade: date (nullable = true)\n",
            " |-- pais: integer (nullable = true)\n",
            " |-- representante_legal: string (nullable = true)\n",
            " |-- nome_do_representante: string (nullable = true)\n",
            " |-- qualificacao_do_representante_legal: integer (nullable = true)\n",
            " |-- faixa_etaria: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "socios.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "Ta9UsinQJn1w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- cnpj_basico: integer (nullable = true)\n",
            " |-- cnpj_ordem: integer (nullable = true)\n",
            " |-- cnpj_dv: integer (nullable = true)\n",
            " |-- identificador_matriz_filial: integer (nullable = true)\n",
            " |-- nome_fantasia: string (nullable = true)\n",
            " |-- situacao_cadastral: integer (nullable = true)\n",
            " |-- data_situacao_cadastral: date (nullable = true)\n",
            " |-- motivo_situacao_cadastral: integer (nullable = true)\n",
            " |-- nome_da_cidade_no_exterior: string (nullable = true)\n",
            " |-- pais: integer (nullable = true)\n",
            " |-- data_de_inicio_atividade: date (nullable = true)\n",
            " |-- cnae_fiscal_principal: integer (nullable = true)\n",
            " |-- cnae_fiscal_secundaria: string (nullable = true)\n",
            " |-- tipo_de_logradouro: string (nullable = true)\n",
            " |-- logradouro: string (nullable = true)\n",
            " |-- numero: string (nullable = true)\n",
            " |-- complemento: string (nullable = true)\n",
            " |-- bairro: string (nullable = true)\n",
            " |-- cep: integer (nullable = true)\n",
            " |-- uf: string (nullable = true)\n",
            " |-- municipio: integer (nullable = true)\n",
            " |-- ddd_1: string (nullable = true)\n",
            " |-- telefone_1: string (nullable = true)\n",
            " |-- ddd_2: string (nullable = true)\n",
            " |-- telefone_2: string (nullable = true)\n",
            " |-- ddd_do_fax: integer (nullable = true)\n",
            " |-- fax: string (nullable = true)\n",
            " |-- correio_eletronico: string (nullable = true)\n",
            " |-- situacao_especial: string (nullable = true)\n",
            " |-- data_da_situacao_especial: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "estabelecimentos.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "kki6ZPfNJ_sI"
      },
      "outputs": [],
      "source": [
        "empresas_join = estabelecimentos.join(empresas, 'cnpj_basico', how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "EIhL8EKGN09L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- cnpj_basico: integer (nullable = true)\n",
            " |-- cnpj_ordem: integer (nullable = true)\n",
            " |-- cnpj_dv: integer (nullable = true)\n",
            " |-- identificador_matriz_filial: integer (nullable = true)\n",
            " |-- nome_fantasia: string (nullable = true)\n",
            " |-- situacao_cadastral: integer (nullable = true)\n",
            " |-- data_situacao_cadastral: date (nullable = true)\n",
            " |-- motivo_situacao_cadastral: integer (nullable = true)\n",
            " |-- nome_da_cidade_no_exterior: string (nullable = true)\n",
            " |-- pais: integer (nullable = true)\n",
            " |-- data_de_inicio_atividade: date (nullable = true)\n",
            " |-- cnae_fiscal_principal: integer (nullable = true)\n",
            " |-- cnae_fiscal_secundaria: string (nullable = true)\n",
            " |-- tipo_de_logradouro: string (nullable = true)\n",
            " |-- logradouro: string (nullable = true)\n",
            " |-- numero: string (nullable = true)\n",
            " |-- complemento: string (nullable = true)\n",
            " |-- bairro: string (nullable = true)\n",
            " |-- cep: integer (nullable = true)\n",
            " |-- uf: string (nullable = true)\n",
            " |-- municipio: integer (nullable = true)\n",
            " |-- ddd_1: string (nullable = true)\n",
            " |-- telefone_1: string (nullable = true)\n",
            " |-- ddd_2: string (nullable = true)\n",
            " |-- telefone_2: string (nullable = true)\n",
            " |-- ddd_do_fax: integer (nullable = true)\n",
            " |-- fax: string (nullable = true)\n",
            " |-- correio_eletronico: string (nullable = true)\n",
            " |-- situacao_especial: string (nullable = true)\n",
            " |-- data_da_situacao_especial: date (nullable = true)\n",
            " |-- razao_social_nome_empresarial: string (nullable = true)\n",
            " |-- natureza_juridica: integer (nullable = true)\n",
            " |-- qualificacao_do_responsavel: integer (nullable = true)\n",
            " |-- capital_social_da_empresa: double (nullable = true)\n",
            " |-- porte_da_empresa: integer (nullable = true)\n",
            " |-- ente_federativo_responsavel: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empresas_join.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "mrIpShtRN3WK"
      },
      "outputs": [],
      "source": [
        "freq = empresas_join.select(\n",
        "    'cnpj_basico', f.year('data_de_inicio_atividade').alias('data_inicio')\n",
        "    ).where('data_inicio >= 2010').groupBy('data_inicio').agg(f.count('cnpj_basico').alias('frequencia')).orderBy('data_inicio', ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "LZ9OYX7NN8SM"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data_inicio</th>\n",
              "      <th>frequencia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010</td>\n",
              "      <td>154159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011</td>\n",
              "      <td>172677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012</td>\n",
              "      <td>232480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013</td>\n",
              "      <td>198424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014</td>\n",
              "      <td>202276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2015</td>\n",
              "      <td>212523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2016</td>\n",
              "      <td>265417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2017</td>\n",
              "      <td>237292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2018</td>\n",
              "      <td>275435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2019</td>\n",
              "      <td>325922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2020</td>\n",
              "      <td>400654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2021</td>\n",
              "      <td>153275</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    data_inicio  frequencia\n",
              "0          2010      154159\n",
              "1          2011      172677\n",
              "2          2012      232480\n",
              "3          2013      198424\n",
              "4          2014      202276\n",
              "5          2015      212523\n",
              "6          2016      265417\n",
              "7          2017      237292\n",
              "8          2018      275435\n",
              "9          2019      325922\n",
              "10         2020      400654\n",
              "11         2021      153275"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "freq.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "FdFVRNJeQRua"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 195:>                                                      (0 + 11) / 11]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----------+\n",
            "|data_inicio|frequencia|\n",
            "+-----------+----------+\n",
            "|       2010|    154159|\n",
            "|       2011|    172677|\n",
            "|       2012|    232480|\n",
            "|       2013|    198424|\n",
            "|       2014|    202276|\n",
            "|       2015|    212523|\n",
            "|       2016|    265417|\n",
            "|       2017|    237292|\n",
            "|       2018|    275435|\n",
            "|       2019|    325922|\n",
            "|       2020|    400654|\n",
            "|       2021|    153275|\n",
            "|      Total|   2830534|\n",
            "+-----------+----------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "freq.union(\n",
        "    freq.select(\n",
        "        f.lit('Total').alias('data_inicio'), \n",
        "        f.sum(freq.frequencia).alias('frequencia')\n",
        "    )\n",
        ").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSEJMOF5PxGF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qARQXzC71hPq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26U30E70nTew"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq-9n_n2GlIF"
      },
      "source": [
        "## SparkSQL\n",
        "\n",
        "[SparkSession.sql(sqlQuery)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.sql.html)\n",
        "\n",
        "Para saber mais sobre performance: [Artigo - Spark RDDs vs DataFrames vs SparkSQL](https://community.cloudera.com/t5/Community-Articles/Spark-RDDs-vs-DataFrames-vs-SparkSQL/ta-p/246547)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "NGCFVpPnGlIF"
      },
      "outputs": [],
      "source": [
        "empresas.createOrReplaceTempView('empresasView')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "5c_OrvMLGlIF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------------------------------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
            "|cnpj_basico|razao_social_nome_empresarial                                                               |natureza_juridica|qualificacao_do_responsavel|capital_social_da_empresa|porte_da_empresa|ente_federativo_responsavel|\n",
            "+-----------+--------------------------------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
            "|306        |FRANCAMAR REFRIGERACAO TECNICA S/C LTDA                                                     |2240             |49                         |0.0                      |1               |NULL                       |\n",
            "|1355       |BRASILEIRO & OLIVEIRA LTDA                                                                  |2062             |49                         |0.0                      |5               |NULL                       |\n",
            "|4820       |REGISTRO DE IMOVEIS, TABELIONATO 1 DE NOTAS E TABELIONATO E REGISTRO DE CONSTRATOS MARITIMOS|3034             |32                         |0.0                      |5               |NULL                       |\n",
            "|5347       |ROSELY APARECIDA MONTEIRO CALTABIANO FREITAS                                                |2135             |50                         |0.0                      |5               |NULL                       |\n",
            "|6846       |BADU E FILHOS TECIDOS LTDA                                                                  |2062             |49                         |4000.0                   |1               |NULL                       |\n",
            "+-----------+--------------------------------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('SELECT * FROM empresasView').show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "5cff25Y3GlIG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
            "|cnpj_basico|razao_social_nome_empresarial|natureza_juridica|qualificacao_do_responsavel|capital_social_da_empresa|porte_da_empresa|ente_federativo_responsavel|\n",
            "+-----------+-----------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
            "|   17350147|         ERIK MARCELO DOS ...|             2135|                         50|                     50.0|               1|                       NULL|\n",
            "|   17833214|         ALEXANDRE MACHADO...|             2135|                         50|                     50.0|               1|                       NULL|\n",
            "|   20860830|         YASMIN MOURA DA F...|             2135|                         50|                     50.0|               1|                       NULL|\n",
            "|   22242856|         JOAO CESAR MESSIA...|             2135|                         50|                     50.0|               1|                       NULL|\n",
            "|   23238540|         EVERTON ROBERTO D...|             2135|                         50|                     50.0|               1|                       NULL|\n",
            "+-----------+-----------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "    SELECT * FROM empresasView\n",
        "           WHERE capital_social_da_empresa = 50\n",
        "\"\"\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "Rj0ADzBfGlIG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 219:>                                                      (0 + 10) / 10]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+------------------+\n",
            "|porte_da_empresa|             Media|\n",
            "+----------------+------------------+\n",
            "|            NULL|  8.35421888053467|\n",
            "|               1|339994.53313507047|\n",
            "|               3|2601001.7677092687|\n",
            "|               5| 708660.4208249793|\n",
            "+----------------+------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "    SELECT porte_da_empresa, MEAN(capital_social_da_empresa) as Media FROM empresasView\n",
        "           GROUP BY porte_da_empresa\n",
        "\"\"\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "RRxV3Crg1ZFo"
      },
      "outputs": [],
      "source": [
        "empresas_join.createOrReplaceTempView('empresasJoinView')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "Q0zjZOxM0jzy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 232:===========>                                            (2 + 8) / 10]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----------+\n",
            "|data_inicio|frequencia|\n",
            "+-----------+----------+\n",
            "|       2010|    154159|\n",
            "|       2011|    172677|\n",
            "|       2012|    232480|\n",
            "|       2013|    198424|\n",
            "|       2014|    202276|\n",
            "|       2015|    212523|\n",
            "|       2016|    265417|\n",
            "|       2017|    237292|\n",
            "|       2018|    275435|\n",
            "|       2019|    325922|\n",
            "|       2020|    400654|\n",
            "|       2021|    153275|\n",
            "+-----------+----------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "freq = spark.sql(\"\"\"\n",
        "    SELECT YEAR(data_de_inicio_atividade) as data_inicio, COUNT(cnpj_basico) as frequencia\n",
        "              FROM empresasJoinView\n",
        "              WHERE YEAR(data_de_inicio_atividade) >= 2010\n",
        "              GROUP BY YEAR(data_de_inicio_atividade)\n",
        "              ORDER BY YEAR(data_de_inicio_atividade) ASC\n",
        "\"\"\")\n",
        "\n",
        "freq.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "Z0oUrvo4E76z"
      },
      "outputs": [],
      "source": [
        "freq.createOrReplaceTempView('freqView')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "ghAEAvfj0eS3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 244:>                                                      (0 + 11) / 11]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----------+\n",
            "|data_inicio|frequencia|\n",
            "+-----------+----------+\n",
            "|       2010|    154159|\n",
            "|       2011|    172677|\n",
            "|       2012|    232480|\n",
            "|       2013|    198424|\n",
            "|       2014|    202276|\n",
            "|       2015|    212523|\n",
            "|       2016|    265417|\n",
            "|       2017|    237292|\n",
            "|       2018|    275435|\n",
            "|       2019|    325922|\n",
            "|       2020|    400654|\n",
            "|       2021|    153275|\n",
            "|      Total|   2830534|\n",
            "+-----------+----------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "          SELECT * FROM freqView\n",
        "          UNION ALL \n",
        "          SELECT 'Total' as data_inicio, SUM(frequencia) as frequencia from freqView\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDM3l-CUGlIJ"
      },
      "source": [
        "# Formas de Armazenamento\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySrkXajNGlIK"
      },
      "source": [
        "## Arquivos CSV\n",
        "\n",
        "[property DataFrame.write](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.write.html)\n",
        "\n",
        "[DataFrameWriter.csv(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.csv.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "f1PBgc6kGlIK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "empresas.write.csv(path='data_refined/empresas', mode='overwrite', sep=';', header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42_1Y8eW6e2r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD01X0MN6pgp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4KYGLOUe9VN"
      },
      "source": [
        "## Faça como eu fiz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "8ttmS8v1GlIK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "socios.write.csv(path='data_refined/socios', mode='overwrite', sep=';', header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "3YvAdU_MGlIK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "estabelecimentos.write.csv(path='data_refined/estabelecimentos', mode='overwrite', sep=';', header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge7vmWujGlIK"
      },
      "source": [
        "## Arquivos PARQUET\n",
        "\n",
        "[Apache Parquet](https://parquet.apache.org/)\n",
        "\n",
        "[DataFrameWriter.parquet(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.parquet.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "qd14cXcto9za"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/08/13 18:33:33 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:33:33 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:33:33 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 76,00% for 10 writers\n",
            "24/08/13 18:33:35 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:33:35 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "empresas.write.parquet(path='data_refined/empresas', mode='overwrite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "ZysCHmzicRo4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/08/13 18:34:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:34:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:34:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 76,00% for 10 writers\n",
            "24/08/13 18:34:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 69,09% for 11 writers\n",
            "24/08/13 18:34:59 ERROR Utils: Aborting task                      (0 + 11) / 11]\n",
            "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "24/08/13 18:34:59 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 76,00% for 10 writers\n",
            "24/08/13 18:34:59 ERROR FileFormatWriter: Job job_202408131834557219578009365260150_0271 aborted.\n",
            "24/08/13 18:34:59 ERROR Executor: Exception in task 2.0 in stage 271.0 (TID 850)\n",
            "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "24/08/13 18:34:59 WARN TaskSetManager: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "\n",
            "24/08/13 18:34:59 ERROR TaskSetManager: Task 2 in stage 271.0 failed 1 times; aborting job\n",
            "24/08/13 18:34:59 ERROR Utils: Aborting task\n",
            "org.apache.spark.TaskKilledException\n",
            "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:128)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "24/08/13 18:34:59 ERROR Utils: Aborting task\n",
            "org.apache.spark.TaskKilledException\n",
            "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:128)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "24/08/13 18:34:59 ERROR Utils: Aborting task\n",
            "org.apache.spark.TaskKilledException\n",
            "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:128)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "24/08/13 18:34:59 ERROR Utils: Aborting task\n",
            "org.apache.spark.TaskKilledException\n",
            "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:128)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "24/08/13 18:34:59 ERROR Utils: Aborting task\n",
            "org.apache.spark.TaskKilledException\n",
            "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:128)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "24/08/13 18:34:59 ERROR FileFormatWriter: Aborting job ea1a377f-a180-48dd-87bd-a4ece27e9e5d.\n",
            "org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 271.0 failed 1 times, most recent failure: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
            "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n",
            "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\t... 1 more\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "24/08/13 18:34:59 ERROR Utils: Aborting task\n",
            "org.apache.spark.TaskKilledException\n",
            "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:128)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "24/08/13 18:34:59 ERROR Utils: Aborting task\n",
            "org.apache.spark.TaskKilledException\n",
            "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:128)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "24/08/13 18:34:59 ERROR Utils: Aborting task\n",
            "org.apache.spark.TaskKilledException\n",
            "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:128)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "24/08/13 18:34:59 ERROR Utils: Aborting task\n",
            "org.apache.spark.TaskKilledException\n",
            "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:128)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "24/08/13 18:34:59 ERROR Utils: Aborting task\n",
            "org.apache.spark.TaskKilledException\n",
            "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:128)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "24/08/13 18:34:59 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:34:59 WARN FileOutputCommitter: Could not delete file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos/_temporary/0/_temporary/attempt_202408131834557219578009365260150_0271_m_000005_853\n",
            "24/08/13 18:34:59 ERROR FileFormatWriter: Job job_202408131834557219578009365260150_0271 aborted.\n",
            "24/08/13 18:34:59 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:34:59 WARN FileOutputCommitter: Could not delete file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos/_temporary/0/_temporary/attempt_202408131834557219578009365260150_0271_m_000010_858\n",
            "24/08/13 18:34:59 ERROR FileFormatWriter: Job job_202408131834557219578009365260150_0271 aborted.\n",
            "24/08/13 18:34:59 WARN TaskSetManager: Lost task 5.0 in stage 271.0 (TID 853) (192.168.68.119 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 271.0 failed 1 times, most recent failure: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "\n",
            "Driver stacktrace:)\n",
            "24/08/13 18:34:59 WARN FileOutputCommitter: Could not delete file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos/_temporary/0/_temporary/attempt_202408131834557219578009365260150_0271_m_000009_857\n",
            "24/08/13 18:34:59 ERROR FileFormatWriter: Job job_202408131834557219578009365260150_0271 aborted.\n",
            "24/08/13 18:34:59 WARN FileOutputCommitter: Could not delete file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos/_temporary/0/_temporary/attempt_202408131834557219578009365260150_0271_m_000003_851\n",
            "24/08/13 18:34:59 ERROR FileFormatWriter: Job job_202408131834557219578009365260150_0271 aborted.\n",
            "24/08/13 18:34:59 WARN FileOutputCommitter: Could not delete file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos/_temporary/0/_temporary/attempt_202408131834557219578009365260150_0271_m_000006_854\n",
            "24/08/13 18:34:59 ERROR FileFormatWriter: Job job_202408131834557219578009365260150_0271 aborted.\n",
            "24/08/13 18:34:59 WARN TaskSetManager: Lost task 9.0 in stage 271.0 (TID 857) (192.168.68.119 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 271.0 failed 1 times, most recent failure: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "\n",
            "Driver stacktrace:)\n",
            "24/08/13 18:34:59 WARN TaskSetManager: Lost task 10.0 in stage 271.0 (TID 858) (192.168.68.119 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 271.0 failed 1 times, most recent failure: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "\n",
            "Driver stacktrace:)\n",
            "24/08/13 18:34:59 WARN FileOutputCommitter: Could not delete file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos/_temporary/0/_temporary/attempt_202408131834557219578009365260150_0271_m_000004_852\n",
            "24/08/13 18:34:59 ERROR FileFormatWriter: Job job_202408131834557219578009365260150_0271 aborted.\n",
            "24/08/13 18:34:59 WARN TaskSetManager: Lost task 6.0 in stage 271.0 (TID 854) (192.168.68.119 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 271.0 failed 1 times, most recent failure: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "\n",
            "Driver stacktrace:)\n",
            "24/08/13 18:34:59 WARN FileOutputCommitter: Could not delete file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos/_temporary/0/_temporary/attempt_202408131834557219578009365260150_0271_m_000001_849\n",
            "24/08/13 18:34:59 ERROR FileFormatWriter: Job job_202408131834557219578009365260150_0271 aborted.\n",
            "24/08/13 18:34:59 WARN TaskSetManager: Lost task 3.0 in stage 271.0 (TID 851) (192.168.68.119 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 271.0 failed 1 times, most recent failure: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "\n",
            "Driver stacktrace:)\n",
            "24/08/13 18:34:59 WARN TaskSetManager: Lost task 4.0 in stage 271.0 (TID 852) (192.168.68.119 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 271.0 failed 1 times, most recent failure: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "\n",
            "Driver stacktrace:)\n",
            "24/08/13 18:34:59 WARN TaskSetManager: Lost task 1.0 in stage 271.0 (TID 849) (192.168.68.119 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 271.0 failed 1 times, most recent failure: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "\n",
            "Driver stacktrace:)\n",
            "24/08/13 18:34:59 WARN FileOutputCommitter: Could not delete file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos/_temporary/0/_temporary/attempt_202408131834557219578009365260150_0271_m_000007_855\n",
            "24/08/13 18:34:59 ERROR FileFormatWriter: Job job_202408131834557219578009365260150_0271 abo"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o1208.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 271.0 failed 1 times, most recent failure: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\nwriting dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\ninto Parquet files can be dangerous, as the files may be read by Spark 2.x\nor legacy versions of Hive later, which uses a legacy hybrid calendar that\nis different from Spark 3.0+'s Proleptic Gregorian calendar. See more\ndetails in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\ndatetime values w.r.t. the calendar difference during writing, to get maximum\ninteroperability. Or set the config to \"CORRECTED\" to write the datetime\nvalues as it is, if you are sure that the written files will only be read by\nSpark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\nwriting dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\ninto Parquet files can be dangerous, as the files may be read by Spark 2.x\nor legacy versions of Hive later, which uses a legacy hybrid calendar that\nis different from Spark 3.0+'s Proleptic Gregorian calendar. See more\ndetails in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\ndatetime values w.r.t. the calendar difference during writing, to get maximum\ninteroperability. Or set the config to \"CORRECTED\" to write the datetime\nvalues as it is, if you are sure that the written files will only be read by\nSpark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[145], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mestabelecimentos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_refined/estabelecimentos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Workspace/data_science/spark/.venv/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Workspace/data_science/spark/.venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m~/Workspace/data_science/spark/.venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m~/Workspace/data_science/spark/.venv/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1208.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 271.0 failed 1 times, most recent failure: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\nwriting dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\ninto Parquet files can be dangerous, as the files may be read by Spark 2.x\nor legacy versions of Hive later, which uses a legacy hybrid calendar that\nis different from Spark 3.0+'s Proleptic Gregorian calendar. See more\ndetails in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\ndatetime values w.r.t. the calendar difference during writing, to get maximum\ninteroperability. Or set the config to \"CORRECTED\" to write the datetime\nvalues as it is, if you are sure that the written files will only be read by\nSpark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\nwriting dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\ninto Parquet files can be dangerous, as the files may be read by Spark 2.x\nor legacy versions of Hive later, which uses a legacy hybrid calendar that\nis different from Spark 3.0+'s Proleptic Gregorian calendar. See more\ndetails in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\ndatetime values w.r.t. the calendar difference during writing, to get maximum\ninteroperability. Or set the config to \"CORRECTED\" to write the datetime\nvalues as it is, if you are sure that the written files will only be read by\nSpark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "rted.\n",
            "24/08/13 18:34:59 WARN TaskSetManager: Lost task 7.0 in stage 271.0 (TID 855) (192.168.68.119 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 271.0 failed 1 times, most recent failure: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "\n",
            "Driver stacktrace:)\n",
            "24/08/13 18:34:59 WARN FileOutputCommitter: Could not delete file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos/_temporary/0/_temporary/attempt_202408131834557219578009365260150_0271_m_000000_848\n",
            "24/08/13 18:34:59 ERROR FileFormatWriter: Job job_202408131834557219578009365260150_0271 aborted.\n",
            "24/08/13 18:34:59 WARN TaskSetManager: Lost task 0.0 in stage 271.0 (TID 848) (192.168.68.119 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 271.0 failed 1 times, most recent failure: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "\n",
            "Driver stacktrace:)\n",
            "24/08/13 18:34:59 WARN FileOutputCommitter: Could not delete file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos/_temporary/0/_temporary/attempt_202408131834557219578009365260150_0271_m_000008_856\n",
            "24/08/13 18:34:59 ERROR FileFormatWriter: Job job_202408131834557219578009365260150_0271 aborted.\n",
            "24/08/13 18:34:59 WARN TaskSetManager: Lost task 8.0 in stage 271.0 (TID 856) (192.168.68.119 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 271.0 failed 1 times, most recent failure: Lost task 2.0 in stage 271.0 (TID 850) (192.168.68.119 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/paulorodrigues/Workspace/data_science/spark/data_refined/estabelecimentos.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
            "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n",
            "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n",
            "into Parquet files can be dangerous, as the files may be read by Spark 2.x\n",
            "or legacy versions of Hive later, which uses a legacy hybrid calendar that\n",
            "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n",
            "details in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\n",
            "datetime values w.r.t. the calendar difference during writing, to get maximum\n",
            "interoperability. Or set the config to \"CORRECTED\" to write the datetime\n",
            "values as it is, if you are sure that the written files will only be read by\n",
            "Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:207)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:184)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:183)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n",
            "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
            "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
            "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
            "\t... 17 more\n",
            "\n",
            "Driver stacktrace:)\n"
          ]
        }
      ],
      "source": [
        "estabelecimentos.write.parquet(path='data_refined/estabelecimentos', mode='overwrite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "9siUUauWdAej"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/08/13 18:35:06 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:35:06 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:35:06 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 76,00% for 10 writers\n",
            "24/08/13 18:35:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:35:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "socios.write.parquet(path='data_refined/socios', mode='overwrite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYsfCfudGlIK"
      },
      "source": [
        "## Particionamento dos dados\n",
        "\n",
        "[DataFrameWriter.partitionBy(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.partitionBy.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "p3lGYb4qRVz3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "empresas.coalesce(1).write.parquet(path='data_refined/empresas', mode='overwrite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "i6Tv9DelGlIL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/08/13 18:39:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:39:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:39:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 76,00% for 10 writers\n",
            "24/08/13 18:39:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:39:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:39:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:39:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:39:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 76,00% for 10 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 76,00% for 10 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 84,44% for 9 writers\n",
            "24/08/13 18:39:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "empresas.write.parquet(path='data_refined/empresas', mode='overwrite', partitionBy='porte_da_empresa')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "zFAQ-XlHScMr"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dveTyKd1VitH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
